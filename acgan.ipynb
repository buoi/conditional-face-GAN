{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "acgan",
      "provenance": [],
      "collapsed_sections": [
        "G3ZWc_CToZiK"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "35b16d6478e949dab2894038ad6466ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6eb597de94564b62a4c5593b083e80bd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0dc687643496484ba1a845b13c47bc37",
              "IPY_MODEL_6cca2ada0ddd4c1c892a9e38893de746"
            ]
          }
        },
        "6eb597de94564b62a4c5593b083e80bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0dc687643496484ba1a845b13c47bc37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d7b1f5b7b54c431994797534f2d2404c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43da02e787cf4bceb241faba17c0c41c"
          }
        },
        "6cca2ada0ddd4c1c892a9e38893de746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_904d7b1f8613409a856a7654e48d8a63",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [01:15&lt;00:00,  1.33it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4278c8fea1d04dbd91ec135d711585ea"
          }
        },
        "d7b1f5b7b54c431994797534f2d2404c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43da02e787cf4bceb241faba17c0c41c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "904d7b1f8613409a856a7654e48d8a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4278c8fea1d04dbd91ec135d711585ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2dc6ef9a462c461ba5fc98c5b02593e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7af885cd2d74461cbd4bf8a08499ed5c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_58e3495eff504ac7985f59a7ae02c8d1",
              "IPY_MODEL_af45c95b51844bc9ba6c2e283463e450"
            ]
          }
        },
        "7af885cd2d74461cbd4bf8a08499ed5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58e3495eff504ac7985f59a7ae02c8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_d340bc06f0c44a748255aa4154f1a032",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 786.17MB of 786.17MB uploaded (17.47MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c6164f1463d478e986a0ef76ce94567"
          }
        },
        "af45c95b51844bc9ba6c2e283463e450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_844a1c8dab564b11a789fd5408ef1a5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7828a11fab7940c68224508eb41d468b"
          }
        },
        "d340bc06f0c44a748255aa4154f1a032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c6164f1463d478e986a0ef76ce94567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "844a1c8dab564b11a789fd5408ef1a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7828a11fab7940c68224508eb41d468b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "468ac7f2109d473e9f25ac7d47384d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_32a734d2ec1844f6826662dd3f718fea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e6ce0bb360e9416dafac097c3a79239a",
              "IPY_MODEL_54fa36be2931419386e5541abae9d0e7"
            ]
          }
        },
        "32a734d2ec1844f6826662dd3f718fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6ce0bb360e9416dafac097c3a79239a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7c939aadebb2478890617df178b4da9c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f3ca42cf5d94126b9a3a8cc3d2bb6e9"
          }
        },
        "54fa36be2931419386e5541abae9d0e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_97b4fbfb42354333b3a7885228c1e463",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [19:08&lt;00:00, 11.48s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52c65a9ffd834f97a1ce950fb0db60b2"
          }
        },
        "7c939aadebb2478890617df178b4da9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f3ca42cf5d94126b9a3a8cc3d2bb6e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97b4fbfb42354333b3a7885228c1e463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52c65a9ffd834f97a1ce950fb0db60b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4decd1a3377b45db8bc0158a6e402a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_428097e2dfe94705aa4a9ba6b39f999d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0662f49e1fd345c08025fad632635b87",
              "IPY_MODEL_48d0eb444b1b44058b4051843b7e960d"
            ]
          }
        },
        "428097e2dfe94705aa4a9ba6b39f999d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0662f49e1fd345c08025fad632635b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c7ce83b1bf304eecbe1d5fdc0aca8a8d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_86b748d1dd724f2b987df9835cedb97c"
          }
        },
        "48d0eb444b1b44058b4051843b7e960d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0475640ee254fae9c6cce292a7950b6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [00:33&lt;00:00,  2.95it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c5e17560913453d99bbb1b598300005"
          }
        },
        "c7ce83b1bf304eecbe1d5fdc0aca8a8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "86b748d1dd724f2b987df9835cedb97c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0475640ee254fae9c6cce292a7950b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c5e17560913453d99bbb1b598300005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9ce18f0df3247cbbdd2623b41524e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9bcf183c03c94a22923d7ac7299b25d6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2311c360e1b1463a83a2d1e89ca62dfa",
              "IPY_MODEL_916db396ddb84d26899b185b65e497b7"
            ]
          }
        },
        "9bcf183c03c94a22923d7ac7299b25d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2311c360e1b1463a83a2d1e89ca62dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a0e66c8781a14da085e3510e90c38367",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a4fa479591b44f39613adeef52d8ce0"
          }
        },
        "916db396ddb84d26899b185b65e497b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e61798198b474033933c49af13c2212e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [00:33&lt;00:00,  2.96it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ceb8067d29454e4794744c3bf1cead79"
          }
        },
        "a0e66c8781a14da085e3510e90c38367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a4fa479591b44f39613adeef52d8ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e61798198b474033933c49af13c2212e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ceb8067d29454e4794744c3bf1cead79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buoi/conditional-face-GAN/blob/main/acgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnwd03g75ghA"
      },
      "source": [
        "# DCGAN to generate face images\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2019/04/29<br>\n",
        "**Last modified:** 2021/01/01<br>\n",
        "**Description:** A simple DCGAN trained using `fit()` by overriding `train_step` on CelebA images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMBqo7it5ghH"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "objr1Bp-_5yo",
        "outputId": "344838f6-af62-4405-b2d4-9fa809cf73a9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 31 20:50:04 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    36W / 250W |   6173MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoSAUAUGF2_Q"
      },
      "source": [
        "Select the path for current run images, pay attention, old images in the path will be overwritten!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3ZWc_CToZiK"
      },
      "source": [
        "## Drive mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku2AFHmW4k84"
      },
      "source": [
        "if False:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvpatjzrKha4"
      },
      "source": [
        "# for logging train images\n",
        "if False:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    #runpath = \"/content/drive/MyDrive/progetto_asperti/run2\"\n",
        "    #os.makedirs(runpath)\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOswoZB0oeA2"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RSWEMo35ghH"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import linalg\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Layer, Input, Dense, Reshape, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, ReLU, LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout, Embedding, Concatenate, Add\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, UpSampling2D, BatchNormalization\n",
        "\n",
        "\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import random \n",
        "\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile #chose this or !unzip\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kbuDuH25ghI"
      },
      "source": [
        "## Prepare CelebA data\n",
        "\n",
        "We'll use face images from the CelebA dataset, resized to 64x64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VNPNSKA5ghI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a2ec02-12e1-4783-c0c2-3f5477d3d324"
      },
      "source": [
        "try:\n",
        "    os.makedirs(\"celeba_gan\")\n",
        "    url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "    output = \"celeba_gan/img_align_celeba.zip\"\n",
        "    gdown.download(url, output, quiet=True)\n",
        "\n",
        "    with ZipFile(\"celeba_gan/img_align_celeba.zip\", \"r\") as zipobj:\n",
        "        zipobj.extractall(\"celeba_gan\")\n",
        "        \n",
        "except FileExistsError:\n",
        "    print(\"Already downloaded\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enz5-UMjB06Z"
      },
      "source": [
        "Download face attributes hosted on public \n",
        "github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rSS3wtw6pHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb735d0b-dddd-4b88-d52d-2fbd3d48fcef"
      },
      "source": [
        "USE_LABELS = True\n",
        "if USE_LABELS:\n",
        "    #!wget -q -O \"/content/celeba_gan/list_attr_celeba.txt.zip\" \"https://github.com/buoi/conditional-face-GAN/blob/main/list_attr_celeba.txt.zip?raw=true\" \n",
        "    #!unzip \"/content/celeba_gan/list_attr_celeba.txt.zip\" -d \"/content/celeba_gan\"\n",
        "    #!wget -q -O \"/content/celeba_gan/list_attr_celeba.csv.zip\" \"https://github.com/buoi/conditional-face-GAN/blob/main/list_attr_celeba.csv.zip?raw=true\" \n",
        "    #!unzip \"/content/celeba_gan/list_attr_celeba.csv.zip\" -d \"/content/celeba_gan\"\n",
        "    !wget -q -O \"/content/celeba_gan/list_attr_celeba01.csv.zip\" \"https://github.com/buoi/conditional-face-GAN/blob/main/list_attr_celeba01.csv.zip?raw=true\" \n",
        "    !unzip \"/content/celeba_gan/list_attr_celeba01.csv.zip\" -d \"/content/celeba_gan\"\n",
        "    "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/celeba_gan/list_attr_celeba01.csv.zip\n",
            "replace /content/celeba_gan/list_attr_celeba01.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lguFA8Wg5ghJ"
      },
      "source": [
        "# Keras Dataset \n",
        "create Dataset object from our folder, and rescale the images to the [0-1] range:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS5FRkOd6Gx5"
      },
      "source": [
        "# image utils functions\n",
        "def conv_range(in_range=(-1,1), out_range=(0,255)):\n",
        "    \"\"\" Returns range conversion function\"\"\"\n",
        "\n",
        "    # compute means and spans once\n",
        "    in_mean, out_mean = np.mean(in_range), np.mean(out_range)\n",
        "    in_span, out_span = np.ptp(in_range), np.ptp(out_range)\n",
        "\n",
        "    # return function\n",
        "    def convert_img_range(in_img):\n",
        "        out_img = (in_img - in_mean) / in_span\n",
        "        out_img = out_img * out_span + out_mean\n",
        "        return out_img\n",
        "\n",
        "    return convert_img_range\n",
        "\n",
        "def crop128(img):\n",
        "    #return img[:, 77:141, 57:121]# 64,64 center crop\n",
        "    return img[:, 45:173, 25:153]# 128,128 center crop\n",
        "\n",
        "def resize64(img):\n",
        "    return tf.image.resize(img, (64,64), antialias=True)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "PhjhUVvx5ghJ",
        "outputId": "dd8168f8-5263-422c-fd38-3482bc38c608"
      },
      "source": [
        "\"\"\"BATCH_SIZE = 16\n",
        "\n",
        "# load image at exact resolution\n",
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=None, image_size=(218, 178), \n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "# dataset_01 and dataset_11 fit images in the (0,1) or (-1,1) range respectively\n",
        "\n",
        "dataset_01 = dataset.map(conv_range([0,255], [0,1])).map(crop128).map(resize64)\n",
        "#dataset_11 = dataset.map(range01to11).map(crop128).map(resize64)\n",
        "\n",
        "test_img = next(iter(dataset_01))[0]\n",
        "\n",
        "# plt.imshow expects float images to be in (0,1) and int images in (0,255)\n",
        "plt.imshow((test_img.numpy()))\n",
        "\n",
        "f\"{dataset_01.element_spec} in range: {np.min(test_img)} {np.max(test_img)}\"\"\""
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'BATCH_SIZE = 16\\n\\n# load image at exact resolution\\ndataset = keras.preprocessing.image_dataset_from_directory(\\n    \"celeba_gan\", label_mode=None, image_size=(218, 178), \\n    batch_size=BATCH_SIZE)\\n\\n# dataset_01 and dataset_11 fit images in the (0,1) or (-1,1) range respectively\\n\\ndataset_01 = dataset.map(conv_range([0,255], [0,1])).map(crop128).map(resize64)\\n#dataset_11 = dataset.map(range01to11).map(crop128).map(resize64)\\n\\ntest_img = next(iter(dataset_01))[0]\\n\\n# plt.imshow expects float images to be in (0,1) and int images in (0,255)\\nplt.imshow((test_img.numpy()))\\n\\nf\"{dataset_01.element_spec} in range: {np.min(test_img)} {np.max(test_img)}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCTYxV38p9W2"
      },
      "source": [
        "class DataSequence(Sequence):\n",
        "    \"\"\"\n",
        "    Keras Sequence object to train a model on larger-than-memory data.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, image_directory, batch_size, mode='train',\n",
        "                 labels = ('Smiling', 'Male'),\n",
        "                 processing_function = lambda x: x):\n",
        "        self.df = df # your pandas dataframe\n",
        "        self.bsz = batch_size # batch size\n",
        "        self.mode = mode # shuffle when in train mode\n",
        "        self.im_dir = image_directory\n",
        "        self.prep_fn = processing_function \n",
        "        self.labels = list(labels)\n",
        "\n",
        "        # Take labels and a list of image locations in memory\n",
        "        self.labels = self.df[self.labels].values\n",
        "        self.im_list = self.df['image_id'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        # compute number of batches to yield\n",
        "        return int(math.ceil(len(self.df) / float(self.bsz)))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffles indexes after each epoch if in training mode\n",
        "        self.indexes = range(len(self.im_list))\n",
        "        if self.mode == 'train':\n",
        "            self.indexes = random.sample(self.indexes, k=len(self.indexes))\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return self.labels[idx * self.bsz: (idx + 1) * self.bsz]\n",
        "\n",
        "    def get_batch_features(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return np.array([plt.imread(os.path.join(self.im_dir,im)) for im in self.im_list[idx * self.bsz: (1 + idx) * self.bsz]])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #batch_x = resize64(crop128(self.get_batch_features(idx)))\n",
        "        batch_x = self.prep_fn(self.get_batch_features(idx))\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "        return batch_x, batch_y\n",
        "\n",
        "df=pd.read_csv(r\"/content/celeba_gan/list_attr_celeba01.csv\")\n",
        "directory = \"/content/celeba_gan/img_align_celeba\"\n",
        "BATCH_SIZE = 16\n",
        "LABELS = ('Smiling', 'Male')\n",
        "\n",
        "def preprocess_function(img):\n",
        "    return conv_range((0,255),(0,1))(resize64(crop128(img)))\n",
        "\n",
        "dataset_df = DataSequence(df, directory, BATCH_SIZE, labels=LABELS, \n",
        "                          processing_function =  preprocess_function)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "CTdSOou7qRtX",
        "outputId": "c89581ab-a8e9-4e0e-e4e7-58e4e2ad91a2"
      },
      "source": [
        "batch = next(iter(dataset_df))\n",
        "test_img = batch[0][0]\n",
        "\n",
        "plt.imshow(test_img)\n",
        "batch[0].shape, batch[1], \"in range: \",np.min(test_img), np.max(test_img)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([16, 64, 64, 3]), array([[1, 0],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 0],\n",
              "        [0, 1],\n",
              "        [0, 1],\n",
              "        [1, 0],\n",
              "        [0, 0],\n",
              "        [1, 0],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 0],\n",
              "        [0, 1],\n",
              "        [1, 1]]), 'in range: ', 0.00030636787, 1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR88BN23cXLJ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZDhzbt6otS_"
      },
      "source": [
        "## Custom Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK_8lLqo8C76"
      },
      "source": [
        "### Spectral Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRo12Rq_5Q5P"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "#epsilon set according to BIGGAN https://arxiv.org/pdf/1809.11096.pdf\n",
        "\n",
        "def _l2normalizer(v, epsilon=1e-4):\n",
        "    return v / (K.sum(v**2)**0.5 + epsilon)\n",
        "\n",
        "\n",
        "def power_iteration(W, u, rounds=1):\n",
        "    '''\n",
        "    According to the paper, we only need to do power iteration one time.\n",
        "    '''\n",
        "    _u = u\n",
        "\n",
        "    for i in range(rounds):\n",
        "        _v = _l2normalizer(K.dot(_u, W))\n",
        "        _u = _l2normalizer(K.dot(_v, K.transpose(W)))\n",
        "\n",
        "    W_sn = K.sum(K.dot(_u, W) * _v)\n",
        "    return W_sn, _u, _v\n",
        "\n",
        "\"\"\"\n",
        "Convolution 2D with spectral normalization\n",
        "\"\"\"\n",
        "class SNConv2D(Conv2D):\n",
        "    def __init__(self, filters, spectral_normalization=True, **kwargs):\n",
        "        super(SNConv2D, self).__init__(filters, **kwargs)\n",
        "        self.spectral_normalization = spectral_normalization\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.u = self.add_weight(name='u', shape=(1, self.filters),\n",
        "                                 initializer='uniform', trainable=False)\n",
        "        \n",
        "        super(SNConv2D, self).build(input_shape)\n",
        "        # Be sure to call this at the end\n",
        "\n",
        "    def compute_spectral_normal(self, training=True):\n",
        "        # Spectrally Normalized Weight\n",
        "        if self.spectral_normalization:\n",
        "            # Get kernel tensor shape [kernel_h, kernel_w, in_channels, out_channels]\n",
        "            W_shape = self.kernel.shape.as_list()\n",
        "\n",
        "            # Flatten the Tensor\n",
        "            # [out_channels, N]\n",
        "            W_mat = K.reshape(self.kernel, [W_shape[-1], -1])\n",
        "\n",
        "            W_sn, u, v = power_iteration(W_mat, self.u)\n",
        "\n",
        "            if training:\n",
        "                # Update estimated 1st singular vector\n",
        "                self.u.assign(u)\n",
        "\n",
        "            return self.kernel / W_sn\n",
        "        else:\n",
        "            return self.kernel\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "\n",
        "        outputs = K.conv2d(inputs,\n",
        "                           self.compute_spectral_normal(training=training),\n",
        "                           strides=self.strides, padding=self.padding,\n",
        "                           data_format=self.data_format,\n",
        "                           dilation_rate=self.dilation_rate)\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(outputs, self.bias,\n",
        "                                 data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return super(SNConv2D, self).compute_output_shape(input_shape)\n",
        "\n",
        "\"\"\"\n",
        "Transposed Convolution 2D with spectral normalization\n",
        "\"\"\"\n",
        "class SNConv2DTranspose(Conv2DTranspose):\n",
        "    def __init__(self, spectral_normalization=True, **kwargs):\n",
        "        super(SNConv2DTranspose, self).__init__(**kwargs)\n",
        "        self.spectral_normalization = spectral_normalization\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self.u = self.add_weight(name='u', shape=(1, self.filters),\n",
        "                                 initializer='uniform', trainable=False)\n",
        "        super(SNConv2DTranspose, self).build(input_shape)\n",
        "        # Be sure to call this at the end\n",
        "\n",
        "    def compute_spectral_normal(self, training=True):\n",
        "        # Spectrally Normalized Weight\n",
        "        if self.spectral_normalization:\n",
        "            # Get kernel tensor shape [kernel_h, kernel_w, in_channels, out_channels]\n",
        "            W_shape = self.kernel.shape.as_list()\n",
        "\n",
        "            # Flatten the Tensor\n",
        "            # [out_channels, N]\n",
        "            W_mat = K.reshape(self.kernel, [W_shape[-2], -1])\n",
        "\n",
        "            W_sn, u, v = power_iteration(W_mat, self.u)\n",
        "\n",
        "            if training:\n",
        "                # Update estimated 1st singular vector\n",
        "                self.u.assign(u)\n",
        "\n",
        "            return self.kernel / W_sn\n",
        "        else:\n",
        "            return self.kernel\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = K.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        if self.data_format == 'channels_first':\n",
        "            h_axis, w_axis = 2, 3\n",
        "        else:\n",
        "            h_axis, w_axis = 1, 2\n",
        "\n",
        "        height, width = input_shape[h_axis], input_shape[w_axis]\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "        stride_h, stride_w = self.strides\n",
        "        out_pad_h = out_pad_w = None\n",
        "\n",
        "        # Infer the dynamic output shape:\n",
        "        out_height = conv_utils.deconv_output_length(\n",
        "            height, kernel_h, self.padding, stride=stride_h)\n",
        "        out_width = conv_utils.deconv_output_length(\n",
        "            width, kernel_w, self.padding, stride=stride_w)\n",
        "        if self.data_format == 'channels_first':\n",
        "            output_shape = (batch_size, self.filters, out_height, out_width)\n",
        "        else:\n",
        "            output_shape = (batch_size, out_height, out_width, self.filters)\n",
        "\n",
        "        outputs = K.conv2d_transpose(\n",
        "            inputs,\n",
        "            self.compute_spectral_normal(training=training),\n",
        "            output_shape,\n",
        "            self.strides,\n",
        "            padding=self.padding,\n",
        "            data_format=self.data_format\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(outputs, self.bias,\n",
        "                                 data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return super(SNConv2DTranspose, self).compute_output_shape(input_shape)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_svRbfo9ca3"
      },
      "source": [
        "### Pixel Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYM03aR25R1Q"
      },
      "source": [
        "class PixelNormalization(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(PixelNormalization, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Calculate square pixel values\n",
        "        values = inputs**2.0\n",
        "        # Calculate the mean pixel values\n",
        "        mean_values = tf.keras.backend.mean(values, axis=-1, keepdims=True)\n",
        "        # Ensure the mean is not zero\n",
        "        mean_values += 1.0e-8\n",
        "        # Calculate the sqrt of the mean squared value (L2 norm)\n",
        "        l2 = tf.keras.backend.sqrt(mean_values)\n",
        "        # Normalize values by the l2 norm\n",
        "        normalized = inputs / l2\n",
        "        return normalized\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "    "
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U0P_F--9fFv"
      },
      "source": [
        "### Minibatch Standard Deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8teOYwQ7bWo"
      },
      "source": [
        "class MinibatchStdev(Layer):\n",
        "    # initialize the layer\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MinibatchStdev, self).__init__(**kwargs)\n",
        "\n",
        "    # perform the operation\n",
        "    def call(self, inputs):\n",
        "        # calculate the mean value for each pixel across channels\n",
        "        mean = tf.keras.backend.mean(inputs, axis=0, keepdims=True)\n",
        "        # calculate the squared differences between pixel values and mean\n",
        "        squ_diffs = tf.keras.backend.square(inputs - mean)\n",
        "        # calculate the average of the squared differences (variance)\n",
        "        mean_sq_diff = tf.keras.backend.mean(squ_diffs, axis=0, keepdims=True)\n",
        "        # add a small value to avoid a blow-up when we calculate stdev\n",
        "        mean_sq_diff += 1e-8\n",
        "        # square root of the variance (stdev)\n",
        "        stdev = tf.keras.backend.sqrt(mean_sq_diff)\n",
        "        # calculate the mean standard deviation across each pixel coord\n",
        "        mean_pix = tf.keras.backend.mean(stdev, keepdims=True)\n",
        "        # scale this up to be the size of one input feature map for each sample\n",
        "        shape = tf.keras.backend.shape(inputs)\n",
        "        output = tf.keras.backend.tile(mean_pix, (shape[0], shape[1], shape[2], 1))\n",
        "        # concatenate with the output\n",
        "        combined = tf.keras.backend.concatenate([inputs, output], axis=-1)\n",
        "        return combined\n",
        "\n",
        "    # define the output shape of the layer\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # create a copy of the input shape as a list\n",
        "        input_shape = list(input_shape)\n",
        "        # add one to the channel dimension (assume channels-last)\n",
        "        input_shape[-1] += 1\n",
        "        # convert list to a tuple\n",
        "        return tuple(input_shape)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EOUGrgT9iZp"
      },
      "source": [
        "## Define Building Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ2c5Sv0WX6r",
        "outputId": "e73fb607-57d6-47f3-daa9-a4d62ee9047f"
      },
      "source": [
        "def define_std_discriminator(n_attributes = 40, wgan=False):\n",
        "  \n",
        "    input = layers.Input(shape=(64, 64, 3))\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\")(input)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    out_realfake = layers.Dense(1)(x)\n",
        "    if wgan:\n",
        "        out_realfake = layers.Activation('tanh',name='tanh_realfake')(out_realfake)\n",
        "    else:\n",
        "        out_realfake = layers.Activation('sigmoid',name='sigmoid_realfake')(out_realfake)\n",
        " \n",
        "    out_label = layers.Dense(n_attributes)(x)\n",
        "\n",
        "    out_label = layers.Activation('sigmoid', name='sigmoid_label')(out_label)\n",
        "\n",
        "    discriminator = keras.models.Model(input, [out_realfake, out_label], \n",
        "                                       name='discriminator'+ ('_wgan' if wgan else ''))\n",
        "\n",
        "    return discriminator\n",
        "\n",
        "def define_std_generator(latent_dim = 128, n_attributes = 40, wgan=False):\n",
        "\n",
        "    input_latent = layers.Input(shape=(latent_dim,))\n",
        "    input_attr = layers.Input(shape=(n_attributes))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_latent, tf.cast(input_attr, dtype='float32')])\n",
        " \n",
        "    x = layers.Dense(8 * 8 * 128)(x)\n",
        "    x = layers.Reshape((8, 8, 128))(x)\n",
        "    x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\")(x)\n",
        "    x =  layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Conv2D(3, kernel_size=5, padding=\"same\")(x)\n",
        "  \n",
        "    if wgan:\n",
        "        x = layers.Activation('tanh',name='tanh')(x)\n",
        "    else:\n",
        "        x = layers.Activation('sigmoid',name='sigmoid')(x)\n",
        "\n",
        "    generator = keras.models.Model([input_latent, input_attr], x, name='generator'+ ('_wgan' if wgan else ''))\n",
        "\n",
        "    return generator\n",
        "\n",
        "discriminator = define_std_discriminator(n_attributes=2)\n",
        "generator = define_std_generator(n_attributes=2)\n",
        "discriminator.summary()\n",
        "generator.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_16 (InputLayer)           [(None, 64, 64, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 32, 32, 64)   3136        input_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, 32, 32, 64)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 128)  131200      leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, 16, 16, 128)  0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 16, 16, 128)  0           leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 128)    262272      dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 8, 8, 128)    0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 8, 8, 128)    0           leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 8192)         0           dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1)            8193        flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 2)            16386       flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "sigmoid_realfake (Activation)   (None, 1)            0           dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sigmoid_label (Activation)      (None, 2)            0           dense_16[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 421,187\n",
            "Trainable params: 421,187\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"generator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_17 (InputLayer)           [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.cast_25 (TFOpLambda)         (None, 2)            0           input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 130)          0           input_17[0][0]                   \n",
            "                                                                 tf.cast_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 8192)         1073152     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 8, 8, 128)    0           dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_15 (Conv2DTran (None, 16, 16, 128)  262272      reshape_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 16, 16, 128)  0           conv2d_transpose_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_16 (Conv2DTran (None, 32, 32, 256)  524544      leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_transpose_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_17 (Conv2DTran (None, 64, 64, 512)  2097664     leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)      (None, 64, 64, 512)  0           conv2d_transpose_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 64, 64, 3)    38403       leaky_re_lu_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "sigmoid (Activation)            (None, 64, 64, 3)    0           conv2d_23[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 3,996,035\n",
            "Trainable params: 3,996,035\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "infXenHC6OWR"
      },
      "source": [
        "# define the standalone discriminator model\n",
        "def define_discriminator(filters=128, kernel_size=4, strides=(2, 2), in_shape=(64,64,3), in_smooth = True, wgan =False):\n",
        "    model = keras.Sequential()\n",
        "    # normal\n",
        "    #model.add(keras.layers.InputLayer(input_shape = in_shape))\n",
        "    #model.add(keras.layers.Lambda(lambda x : x + tf.random.normal(shape=in_shape, mean=0.0, stddev=0.01, dtype=tf.float32)))\n",
        "\n",
        "    model.add(SNConv2D(filters=filters, kernel_size=kernel_size,  strides=strides, padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True, input_shape=in_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # downsample to 32x32\n",
        "    model.add(SNConv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # downsample to 16x16\n",
        "    model.add(SNConv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # downsample to 8x8\n",
        "    model.add(SNConv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # downsample to 4x4\n",
        "    model.add(SNConv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    model.add(MinibatchStdev())\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    \n",
        "    # classifier\n",
        "    model.add(Flatten())\n",
        "    \n",
        "    model.add(Dropout(0.4))\n",
        "    if wgan:\n",
        "        model.add(Dense(1))\n",
        "    else:\n",
        "        model.add(Dense(1), activation='sigmoid')\n",
        "    # compile model\n",
        "    #opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    #model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(latent_dim, filters=128, kernel_size=4, strides=(2, 2)):\n",
        "    model = keras.Sequential()\n",
        "    # foundation for 5x5 feature maps\n",
        "    n_nodes = 128 * 4 * 4\n",
        "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((4, 4, 128)))\n",
        "    \n",
        "    # upsample to 8x8\n",
        "    model.add(SNConv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=False))\n",
        "    model.add(PixelNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    # upsample to 16x16\n",
        "    model.add(SNConv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=False))\n",
        "    model.add(PixelNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    # upsample to 32x32\n",
        "    model.add(SNConv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=False))\n",
        "    model.add(PixelNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    # upsample to 64x64\n",
        "    model.add(SNConv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=False))\n",
        "    model.add(PixelNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    # output layer 64x64x3\n",
        "    model.add(SNConv2D(filters=3, kernel_size=kernel_size, activation='tanh', padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=False))\n",
        "    #model.add(SNConv2D(filters=3, kernel_size=kernel_size, activation='sigmoid', padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=False))\n",
        "    \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miESm-gD5ghL"
      },
      "source": [
        "## Define GAN Class\n",
        "override `train_step`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgKFIfptSl9i",
        "outputId": "26e6e7cc-8f9d-4132-978f-6b5f8a2f4350"
      },
      "source": [
        "\n",
        "# disriminator output for realness\n",
        "pred_realfake = np.array([[0],[0],[0],[100000]])\n",
        "label_realfake = np.array([[0],[0],[0],[0.]])\n",
        "\n",
        "#discriminator output for attributes\n",
        "pred_attr = np.array([[1,1],[1,1],[1,1],[1,1.]])\n",
        "label_attr = np.zeros((4,2),dtype='float32')\n",
        "\n",
        "# should be comparable, at initialization\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "loss_fn(pred_realfake, label_realfake), loss_fn(pred_attr, label_attr)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float64, numpy=385623.71875>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=15.424949>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw5lGLoLBobm"
      },
      "source": [
        "tf.concat([real_attr, real_attr], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDGCdwcH5ghL"
      },
      "source": [
        "class ACGAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim, n_attributes, **kwargs):\n",
        "        super(ACGAN, self).__init__(**kwargs)\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_attributes = n_attributes# deprecated, take from data\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(ACGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "        self.d_realfake_metric = keras.metrics.Mean(name=\"d_realfake\")#\n",
        "        self.d_attr_metric = keras.metrics.Mean(name=\"d_attr\")#\n",
        "\n",
        "        self.g_realfake_metric = keras.metrics.Mean(name=\"g_realfake\")#\n",
        "        self.g_attr_metric = keras.metrics.Mean(name=\"g_attr\")#\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        real_images, real_attr = data \n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        \n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        #random_attr = tf.random.uniform(shape=(batch_size, self.n_attributes), maxval=2, dtype='int64')\n",
        "\n",
        "        # Decode them to fake images\n",
        "        #generated_images = self.generator([random_latent_vectors, random_attr])\n",
        "        generated_images = self.generator([random_latent_vectors, real_attr])\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels_realfake = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
        "\n",
        "        #labels_attr = tf.concat([random_attr, real_attr], axis=0)\n",
        "        # NON HA SENSO LA LOSS SULLA SECONDA PARTE... OPPURE VA INVERTITA IN BASSO\n",
        "        labels_attr = tf.concat([real_attr, real_attr], axis=0)\n",
        "        labels_attr = tf.cast(labels_attr, dtype='float32')\n",
        "\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels_realfake = labels_realfake + 0.05 * tf.random.uniform(tf.shape(labels_realfake))\n",
        "        #labels_attr = labels_attr + 0.05 * tf.random.uniform(tf.shape(labels_attr))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_realfake, pred_attr = self.discriminator(combined_images)\n",
        "\n",
        "            d_loss_realfake = self.loss_fn(labels_realfake, pred_realfake)\n",
        "            #print(f\"loss_fn({labels_realfake}, {pred_realfake})\")\n",
        "            #print('= ',d_loss_realfake)\n",
        "\n",
        "            d_loss_attr = self.loss_fn(labels_attr, pred_attr)#######-1tf.reduce_mean(\n",
        "            #print(f\"loss_fn({labels_attr}, {pred_attr})\")\n",
        "            #print('= ',self.loss_fn(labels_attr, pred_attr))\n",
        "            #print('reduce_mean -> ',d_loss_attr)\n",
        "\n",
        "\n",
        "            #print(f\"d_loss_realfake: {d_loss_realfake} + d_loss_attr: {d_loss_attr}\")\n",
        "            # OPPURE NESSUNA LOSS ATTRIBUTE MENTRE TRAINIAMO IL DISCRIMINATOR?\n",
        "            d_loss = d_loss_realfake + d_loss_attr \n",
        "      \n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        ## GEN\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        #random_attr = tf.random.uniform(shape=(batch_size, self.n_attributes), maxval=2, dtype='int64')\n",
        "\n",
        "        # using sampled real attributes during entire training step\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        ##### SHOULD WE SWITCH ATTRIBUTES?!\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_realfake, pred_attr = self.discriminator(self.generator([random_latent_vectors, real_attr]))\n",
        "            g_loss_realfake = self.loss_fn(misleading_labels, pred_realfake)\n",
        "\n",
        "            #print(f\"loss_fn({misleading_labels}, {pred_realfake})\")\n",
        "            #print('= ',g_loss_realfake)\n",
        "\n",
        "            g_loss_attr = self.loss_fn(real_attr, pred_attr) ##########tf.reduce_mean(\n",
        "            #print(f\"loss_fn({random_attr}, {pred_attr})\")\n",
        "            #print('= ',self.loss_fn(real_attr, pred_attr))\n",
        "            #print('reduce_mean -> ',g_loss_attr)\n",
        "\n",
        "            #print(f\"g_loss_realfake: {g_loss_realfake} + g_loss_attr: {g_loss_attr}\")\n",
        "            g_loss = g_loss_realfake + g_loss_attr\n",
        "\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "\n",
        "        self.d_realfake_metric.update_state(d_loss_realfake)\n",
        "        self.d_attr_metric.update_state(d_loss_attr)\n",
        "\n",
        "        self.g_realfake_metric.update_state(g_loss_realfake)\n",
        "        self.g_attr_metric.update_state(g_loss_attr)\n",
        "       \n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "\n",
        "            \"d_loss_realfake\": self.d_realfake_metric.result(),\n",
        "            \"d_loss_attr\": self.d_attr_metric.result(),\n",
        "            \"g_loss_realfake\": self.g_realfake_metric.result(),\n",
        "            \"g_loss_attr\": self.g_attr_metric.result()\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        latent, label = inputs\n",
        "        generated_images = self.generator([latent, label])\n",
        "        d_loss = self.discriminator(generated_images)\n",
        "        return generated_images, d_loss\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQdXIfxBfgFW"
      },
      "source": [
        "class WGAN(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        discriminator,\n",
        "        generator,\n",
        "        latent_dim,\n",
        "        discriminator_extra_steps=3,\n",
        "        gp_weight=10.0,\n",
        "    ):\n",
        "        super(WGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_steps = discriminator_extra_steps\n",
        "        self.gp_weight = gp_weight\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
        "        super(WGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_loss_fn = d_loss_fn\n",
        "        self.g_loss_fn = g_loss_fn\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "        \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "        This loss is calculated on an interpolated image\n",
        "        and added to the discriminator loss.\n",
        "        \"\"\"\n",
        "        # Get the interpolated image\n",
        "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "        diff = fake_images - real_images\n",
        "        interpolated = real_images + alpha * diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            # 1. Get the discriminator output for this interpolated image.\n",
        "            pred = self.discriminator(interpolated, training=True)\n",
        "\n",
        "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        # 3. Calculate the norm of the gradients.\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        return gp\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "\n",
        "        # Get the batch size\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        # For each batch, we are going to perform the\n",
        "        # following steps as laid out in the original paper:\n",
        "        # 1. Train the generator and get the generator loss\n",
        "        # 2. Train the discriminator and get the discriminator loss\n",
        "        # 3. Calculate the gradient penalty\n",
        "        # 4. Multiply this gradient penalty with a constant weight factor\n",
        "        # 5. Add the gradient penalty to the discriminator loss\n",
        "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
        "\n",
        "        # Train the discriminator first. The original paper recommends training\n",
        "        # the discriminator for `x` more steps (typically 5) as compared to\n",
        "        # one step of the generator. Here we will train it for 3 extra steps\n",
        "        # as compared to 5 to reduce the training time.\n",
        "        for i in range(self.d_steps):\n",
        "            # Get the latent vector\n",
        "            random_latent_vectors = tf.random.normal(\n",
        "                shape=(batch_size, self.latent_dim)\n",
        "            )\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Generate fake images from the latent vector\n",
        "                fake_images = self.generator(random_latent_vectors, training=True)\n",
        "                # Get the logits for the fake images\n",
        "                fake_logits = self.discriminator(fake_images, training=True)\n",
        "                # Get the logits for the real images\n",
        "                real_logits = self.discriminator(real_images, training=True)\n",
        "\n",
        "                # Calculate the discriminator loss using the fake and real image logits\n",
        "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
        "                # Calculate the gradient penalty\n",
        "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
        "                # Add the gradient penalty to the original discriminator loss\n",
        "                d_loss = d_cost + gp * self.gp_weight\n",
        "\n",
        "            # Get the gradients w.r.t the discriminator loss\n",
        "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "            # Update the weights of the discriminator using the discriminator optimizer\n",
        "            self.d_optimizer.apply_gradients(\n",
        "                zip(d_gradient, self.discriminator.trainable_variables)\n",
        "            )\n",
        "\n",
        "        # Train the generator\n",
        "        # Get the latent vector\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Generate fake images using the generator\n",
        "            generated_images = self.generator(random_latent_vectors, training=True)\n",
        "            # Get the discriminator logits for fake images\n",
        "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
        "            # Calculate the generator loss\n",
        "            g_loss = self.g_loss_fn(gen_img_logits)\n",
        "\n",
        "        # Get the gradients w.r.t the generator loss\n",
        "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        # Update the weights of the generator using the generator optimizer\n",
        "        self.g_optimizer.apply_gradients(\n",
        "            zip(gen_gradient, self.generator.trainable_variables)\n",
        "        )\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
        "        \n",
        "    def call(self, inputs):\n",
        "      \n",
        "        generated_images = self.generator(inputs)\n",
        "        d_loss = self.discriminator(generated_images)\n",
        "        return generated_images, d_loss"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hpZ9Cju5ghM"
      },
      "source": [
        "## Create callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUCdZvzbbCy1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSMPF5ELnBIv"
      },
      "source": [
        "### Images logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpWnnjZ7a_MN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "986d23b3-22c3-434a-ddbf-df64cf90f8bc"
      },
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=8, latent_dim=128, **kwargs):\n",
        "        super(GANMonitor, self).__init__(**kwargs)\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        random_attr = tf.random.uniform(shape=(self.num_img, N_ATTRIBUTES), maxval=2, dtype='int64')\n",
        "\n",
        "        generated_images = self.model.generator((random_latent_vectors, random_attr))\n",
        "\n",
        "        generated_images.numpy()\n",
        "        if IMAGE_RANGE == '11':\n",
        "            generated_images = conv_range((-1,1), (0,1))(generated_images)\n",
        "\n",
        "        if ENABLE_WANDB:\n",
        "            log_images = [wandb.Image(img) for img in generated_images]\n",
        "            wandb.log({f\"Epoch {epoch}\": (log_images)})\n",
        "\n",
        "\"\"\"    def on_batch_end(self, batch, logs=None):\n",
        "\n",
        "        freq = 2000\n",
        "        if (batch+1) % freq == 0:\n",
        "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "            random_attr = tf.random.uniform(shape=(self.num_img, N_ATTRIBUTES), maxval=2, dtype='int64')\n",
        "\n",
        "            generated_images = self.model.generator((random_latent_vectors, random_attr))\n",
        "            generated_images.numpy()\n",
        "\n",
        "\n",
        "            fig, axes = plt.subplots(1, self.num_img, figsize=(30,30))\n",
        "            for i, axis in enumerate(axes):\n",
        "                axis.axis('off')\n",
        "                axis.title.set_text(f\"{[LABELS[label] for label in random_attr[i]]}\")\n",
        "                if IMAGE_RANGE == '11':\n",
        "                    axis.imshow(range11to01(generated_images[i]))\n",
        "                else:\n",
        "                    axis.imshow(generated_images[i])\n",
        "\n",
        "            fig.set_facecolor((56/255,56/255,56/255))\n",
        "            plt.show()\"\"\""
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'    def on_batch_end(self, batch, logs=None):\\n\\n        freq = 2000\\n        if (batch+1) % freq == 0:\\n            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\\n            random_attr = tf.random.uniform(shape=(self.num_img, N_ATTRIBUTES), maxval=2, dtype=\\'int64\\')\\n\\n            generated_images = self.model.generator((random_latent_vectors, random_attr))\\n            generated_images.numpy()\\n\\n\\n            fig, axes = plt.subplots(1, self.num_img, figsize=(30,30))\\n            for i, axis in enumerate(axes):\\n                axis.axis(\\'off\\')\\n                axis.title.set_text(f\"{[LABELS[label] for label in random_attr[i]]}\")\\n                if IMAGE_RANGE == \\'11\\':\\n                    axis.imshow(range11to01(generated_images[i]))\\n                else:\\n                    axis.imshow(generated_images[i])\\n\\n            fig.set_facecolor((56/255,56/255,56/255))\\n            plt.show()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bLvG6V2bbFA"
      },
      "source": [
        "### Model logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJU9s6nixbc6"
      },
      "source": [
        "# log checkpoint artifacts to wandb\n",
        "\n",
        "SAVE_PATH = 'models/'\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "class WandbLogger(keras.callbacks.Callback):\n",
        "    def __init__(self, model_name, run):\n",
        "        super(WandbLogger, self).__init__()\n",
        "        assert ENABLE_WANDB == True\n",
        "        self.model_name = model_name\n",
        "        self.run = run\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        self.model.save(SAVE_PATH + self.model_name, save_format='tf')\n",
        "        self.artifact = wandb.Artifact(self.model_name, type='model')\n",
        "        self.artifact.add_dir(SAVE_PATH)\n",
        "        self.run.log_artifact(self.artifact)\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaAZrlqRnNe2"
      },
      "source": [
        "### FID Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ085awLWlo-"
      },
      "source": [
        "class FIDLogger(keras.callbacks.Callback):\n",
        "    def __init__(self, real_embeddings, real_labels):\n",
        "        super(FIDLogger, self).__init__()\n",
        "        self.real_embeddings = real_embeddings\n",
        "        self.real_labels = real_labels\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None): # end\n",
        "\n",
        "        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "        generator_prep = define_gen_prep(generator)\n",
        "        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN)\n",
        "        \n",
        "        gen_size = 100\n",
        "        assert FID_COUNT % gen_size == 0\n",
        "\n",
        "        generated_embeddings = np.zeros((FID_COUNT, 2048), dtype='float32')\n",
        "\n",
        "        for i in tqdm(range(FID_COUNT//gen_size)):\n",
        "          \n",
        "            batch_embeddings = generator_prep((\n",
        "                tf.random.normal([gen_size, LATENT_DIM]),\n",
        "                tf.constant(np.squeeze(self.real_labels[i*gen_size:(i+1)*gen_size]))))\n",
        "            \n",
        "            generated_embeddings[i*gen_size:(i+1)*gen_size] = batch_embeddings\n",
        "\n",
        "        fid = compute_fid(self.real_embeddings, generated_embeddings)\n",
        "        print(fid)\n",
        "\n",
        "        if ENABLE_WANDB:\n",
        "            wandb.log({'FID': fid})\n",
        "\n",
        "\n",
        "\"\"\"    def on_batch_end(self, batch, logs=None):\n",
        "        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "        freq = 200\n",
        "        if (batch+1) % freq == 0:\n",
        "            generator_prep = define_gen_prep(self.model.generator)\n",
        "            FID_COUNT = 10000\n",
        "            gen_size = 100\n",
        "            assert FID_COUNT % gen_size == 0\n",
        "\n",
        "            generated_embeddings = np.zeros((FID_COUNT, 2048), dtype='float32')\n",
        "\n",
        "            for i in tqdm(range(FID_COUNT//gen_size)):\n",
        "                batch_embeddings = generator_prep(tf.random.normal([gen_size, LATENT_DIM]))\n",
        "                generated_embeddings[i*gen_size:(i+1)*gen_size] = batch_embeddings\n",
        "\n",
        "            fid = compute_fid(self.real_embeddings, generated_embeddings)\n",
        "            print(fid)\n",
        "        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN)\"\"\"\n",
        "\n",
        "def define_gen_prep(generator):\n",
        "\n",
        "    img = generator.output\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    if IMAGE_RANGE == '01':\n",
        "        img = tf.math.multiply(img,2)\n",
        "        img = tf.math.add(img, -1)\n",
        "\n",
        "    img_up = tf.image.resize(img, (229, 229), method='bilinear', antialias=True, name='upsample_BILINEAR')\n",
        "    output_repr = inception_model(img_up)\n",
        "    generator_prep = tf.keras.Model(inputs=generator.inputs, outputs=[output_repr])\n",
        "\n",
        "    return generator_prep\n",
        "\n",
        "def compute_fid(real_embeddings, generated_embeddings):\n",
        "    # compute mean and covariance statistics\n",
        "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
        "    mu2, sigma2 = generated_embeddings.mean(axis=0), np.cov(generated_embeddings, rowvar=False)\n",
        "    # compute sum squared difference between means\n",
        "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "    # compute sqrt of product between cov\n",
        "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
        "    # check and correct imaginary numbers from sqrt\n",
        "    if np.iscomplexobj(covmean):\n",
        "      covmean = covmean.real\n",
        "    # compute fid score\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    return fid"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxwDwDWtckI2"
      },
      "source": [
        "IMAGE_RANGE = '01' #@param ['01', '11']"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adnl87j8MWGL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "35b16d6478e949dab2894038ad6466ab",
            "6eb597de94564b62a4c5593b083e80bd",
            "0dc687643496484ba1a845b13c47bc37",
            "6cca2ada0ddd4c1c892a9e38893de746",
            "d7b1f5b7b54c431994797534f2d2404c",
            "43da02e787cf4bceb241faba17c0c41c",
            "904d7b1f8613409a856a7654e48d8a63",
            "4278c8fea1d04dbd91ec135d711585ea"
          ]
        },
        "outputId": "ab94ba4b-5187-4fa6-f253-466fb3df57ee"
      },
      "source": [
        "#fid\n",
        "\n",
        "inception_model = InceptionV3(include_top=False, \n",
        "                              weights=\"imagenet\", \n",
        "                              pooling='avg')\n",
        "\n",
        "def define_real_prep():\n",
        "\n",
        "    img = layers.Input(shape=[64,64,3], name='img_input')\n",
        "    img_up = tf.image.resize(img, (229, 229), method='bilinear', antialias=True, name='upsample_BILINEAR')\n",
        "    if IMAGE_RANGE == '01':\n",
        "        img_up = img_up*2 -1\n",
        "\n",
        "    output = inception_model(img_up)\n",
        "\n",
        "    real_prep = tf.keras.Model(inputs=[img], outputs=[output])\n",
        "\n",
        "    return real_prep\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "real_prep = define_real_prep()\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN)\n",
        "\n",
        "FID_COUNT = 10000\n",
        "real_batch_size = 100\n",
        "\n",
        "fid_real_loader = DataSequence(df, directory, real_batch_size, labels=LABELS, \n",
        "                      processing_function =  preprocess_function)\n",
        "    \n",
        "if IMAGE_RANGE == '11':\n",
        "    raise NotImplementedError\n",
        "    \n",
        "real_embeddings = np.zeros((FID_COUNT, 2048), dtype='float32')\n",
        "real_labels = []\n",
        "for i in tqdm(range(FID_COUNT//real_batch_size)):\n",
        "    batch_img, batch_labels = next(iter(fid_real_loader))\n",
        "    batch_embeddings = real_prep(batch_img)\n",
        "\n",
        "    [real_labels.append(label) for label in batch_labels]\n",
        "    real_embeddings[i*real_batch_size:(i+1)*real_batch_size] = batch_embeddings\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35b16d6478e949dab2894038ad6466ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTX1EZbYLy2s"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753,
          "referenced_widgets": [
            "2dc6ef9a462c461ba5fc98c5b02593e7",
            "7af885cd2d74461cbd4bf8a08499ed5c",
            "58e3495eff504ac7985f59a7ae02c8d1",
            "af45c95b51844bc9ba6c2e283463e450",
            "d340bc06f0c44a748255aa4154f1a032",
            "6c6164f1463d478e986a0ef76ce94567",
            "844a1c8dab564b11a789fd5408ef1a5c",
            "7828a11fab7940c68224508eb41d468b"
          ]
        },
        "id": "4gXF5AnjVyQS",
        "outputId": "3a8b07eb-9d77-4d40-c54f-01c429d855fe"
      },
      "source": [
        "ADAM_LR = 0.0001\n",
        "ADAM_B1 = 0.5\n",
        "ADAM_B2 = 0.999\n",
        "BATCH_SIZE = 32\n",
        "N_ATTRIBUTES = 2\n",
        "\n",
        "ENABLE_WANDB = True #@param {type:\"boolean\"}\n",
        "WANDB_RESUME = False #@param {type:\"boolean\"}\n",
        "\n",
        "resume_id = \"21eejnd6\" #@param {type: \"string\"}\n",
        "WANDB_RESUME = (resume_id if WANDB_RESUME else False)\n",
        "\n",
        "project_name = \"GAN\" #@param {type: \"string\"}\n",
        "model_name = \"acgan_std\" #@param {type: \"string\"}\n",
        "experiment_name = \"acgan_std_fixed_train\" #@param {type: \"string\"}\n",
        "run_notes = \"\" #@param {type: \"string\"}\n",
        "\n",
        "assert '-' not in model_name\n",
        "assert ' ' not in model_name\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "if ENABLE_WANDB:\n",
        "    !pip install wandb > /dev/null\n",
        "    !wandb login\n",
        "    import wandb\n",
        "    from wandb.keras import WandbCallback\n",
        "    run = wandb.init(project=project_name, name=experiment_name, resume=WANDB_RESUME)\n",
        "    if run_notes:\n",
        "        wandb.notes = run_notes\n",
        "    wandb.config.batch_size = BATCH_SIZE\n",
        "    wandb.config.adam_lr = ADAM_LR\n",
        "    wandb.config.adam_b1 = ADAM_B1\n",
        "    wandb.config.adam_b2 = ADAM_B2\n",
        "    wandb.config.img_range = IMAGE_RANGE"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbuio\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:3aggjur1) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 934<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2dc6ef9a462c461ba5fc98c5b02593e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 786.14MB of 786.14MB uploaded (17.47MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210531_122544-3aggjur1/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210531_122544-3aggjur1/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>d_loss</td><td>-38243.00391</td></tr><tr><td>g_loss</td><td>489.43353</td></tr><tr><td>_runtime</td><td>18879</td></tr><tr><td>_timestamp</td><td>1622482827</td></tr><tr><td>_step</td><td>19454</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>FID</td><td>654.72993</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>d_loss</td><td>█████████████████████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▃▂▂▁</td></tr><tr><td>g_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▆▇█</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>FID</td><td>▁▁▃▄▅▇█▆▆▇▇▇█▇█</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 121 media file(s), 50 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">acgan_std_fixed_train</strong>: <a href=\"https://wandb.ai/buio/GAN/runs/3aggjur1\" target=\"_blank\">https://wandb.ai/buio/GAN/runs/3aggjur1</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:3aggjur1). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.31<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">acgan_std_fixed_train</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/buio/GAN\" target=\"_blank\">https://wandb.ai/buio/GAN</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/buio/GAN/runs/31a9kjrp\" target=\"_blank\">https://wandb.ai/buio/GAN/runs/31a9kjrp</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210531_214925-31a9kjrp</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlEl_Qbm-CO4"
      },
      "source": [
        "## Build the WGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0TDtPUHgn2n"
      },
      "source": [
        "if False:\n",
        "    # Instantiate the optimizer for both networks\n",
        "    # (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
        "    generator_optimizer = keras.optimizers.Adam(\n",
        "        learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
        "    )\n",
        "    discriminator_optimizer = keras.optimizers.Adam(\n",
        "        learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
        "    )\n",
        "\n",
        "    # Define the loss functions for the discriminator,\n",
        "    # which should be (fake_loss - real_loss).\n",
        "    # We will add the gradient penalty later to this loss function.\n",
        "    def discriminator_loss(real_img, fake_img):\n",
        "        real_loss = tf.reduce_mean(real_img)\n",
        "        fake_loss = tf.reduce_mean(fake_img)\n",
        "        return fake_loss - real_loss\n",
        "\n",
        "\n",
        "    # Define the loss functions for the generator.\n",
        "    def generator_loss(fake_img):\n",
        "        return -tf.reduce_mean(fake_img)\n",
        "\n",
        "    LATENT_DIM = 128\n",
        "\n",
        "    discriminator = define_discriminator(wgan=True)\n",
        "    generator = define_generator(latent_dim = LATENT_DIM)\n",
        "\n",
        "    # Instantiate the WGAN model.\n",
        "    wgan = WGAN(\n",
        "        discriminator=discriminator,\n",
        "        generator=generator,\n",
        "        latent_dim=LATENT_DIM,\n",
        "        discriminator_extra_steps=3,\n",
        "    )\n",
        "\n",
        "    # Compile the WGAN model.\n",
        "    wgan.compile(\n",
        "        d_optimizer=discriminator_optimizer,\n",
        "        g_optimizer=generator_optimizer,\n",
        "        g_loss_fn=generator_loss,\n",
        "        d_loss_fn=discriminator_loss,\n",
        "    )\n",
        "\n",
        "\n",
        "    # build model for saving\n",
        "    images, d_loss = wgan.predict(tf.random.normal(shape=(1, LATENT_DIM)))\n",
        "    plt.imshow(images[0])\n",
        "    print(images.shape, d_loss)\n",
        "    wgan.save('models/'+model_name+experiment_name, save_format='tf')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "627UA8yuH9M_"
      },
      "source": [
        "## Build the ACGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq3HjDpiRkyY",
        "outputId": "6546afc7-24d2-47f6-c49c-8ac21e112147"
      },
      "source": [
        "LATENT_DIM = 128\n",
        "\n",
        "discriminator = define_std_discriminator(n_attributes=N_ATTRIBUTES)\n",
        "generator = define_std_generator(latent_dim=LATENT_DIM, n_attributes=N_ATTRIBUTES)\n",
        "\n",
        "acgan = ACGAN(discriminator, generator, latent_dim=LATENT_DIM, n_attributes=N_ATTRIBUTES)\n",
        "\n",
        "acgan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=ADAM_LR, beta_1=ADAM_B1, beta_2 = ADAM_B2),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=ADAM_LR, beta_1=ADAM_B1, beta_2 = ADAM_B2),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy()#label_smoothing=0.1)\n",
        ")\n",
        "\n",
        "# build model for saving\n",
        "\n",
        "random_latent_vectors = tf.random.normal(shape=(1, 128))\n",
        "random_attr = tf.random.uniform(shape=(1, 2), maxval=2, dtype='int64')\n",
        "\n",
        "images, d_loss = acgan.predict((random_latent_vectors, random_attr))\n",
        "print(images.shape, d_loss, np.min(images), np.max(images))\n",
        "acgan.save('models/'+model_name+experiment_name, save_format='tf')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9d606e8f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(1, 64, 64, 3) [array([[0.50249964]], dtype=float32), array([[0.5023597 , 0.52296716]], dtype=float32)] 0.4926034 0.50508285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgT4VqgKJx-7"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgmKY6jDdvjv",
        "outputId": "abc95acf-c240-4abb-a10a-77769ccc31c4"
      },
      "source": [
        "if WANDB_RESUME:\n",
        "    run_name = 'buio/GAN/'+ model_name + experiment_name\n",
        "    artifact_run = run_name +':latest'\n",
        "\n",
        "    artifact = run.use_artifact(artifact_run, type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "    acgan.load_weights(os.path.join(artifact_dir, model_name + experiment_name))\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact acganacgan_chollet_params:latest, 68.62MB. 8 files... Done. 0:0:0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422,
          "referenced_widgets": [
            "468ac7f2109d473e9f25ac7d47384d9d",
            "32a734d2ec1844f6826662dd3f718fea",
            "e6ce0bb360e9416dafac097c3a79239a",
            "54fa36be2931419386e5541abae9d0e7",
            "7c939aadebb2478890617df178b4da9c",
            "5f3ca42cf5d94126b9a3a8cc3d2bb6e9",
            "97b4fbfb42354333b3a7885228c1e463",
            "52c65a9ffd834f97a1ce950fb0db60b2",
            "4decd1a3377b45db8bc0158a6e402a58",
            "428097e2dfe94705aa4a9ba6b39f999d",
            "0662f49e1fd345c08025fad632635b87",
            "48d0eb444b1b44058b4051843b7e960d",
            "c7ce83b1bf304eecbe1d5fdc0aca8a8d",
            "86b748d1dd724f2b987df9835cedb97c",
            "d0475640ee254fae9c6cce292a7950b6",
            "6c5e17560913453d99bbb1b598300005",
            "d9ce18f0df3247cbbdd2623b41524e49",
            "9bcf183c03c94a22923d7ac7299b25d6",
            "2311c360e1b1463a83a2d1e89ca62dfa",
            "916db396ddb84d26899b185b65e497b7",
            "a0e66c8781a14da085e3510e90c38367",
            "0a4fa479591b44f39613adeef52d8ce0",
            "e61798198b474033933c49af13c2212e",
            "ceb8067d29454e4794744c3bf1cead79"
          ]
        },
        "id": "LaES7uhMhfk-",
        "outputId": "3bafa80b-1e99-41ee-b4d1-47b76d419382"
      },
      "source": [
        "epochs = 200  # In practice, use ~100 epochs\n",
        "\n",
        "acgan.fit(dataset_df, epochs=epochs, \n",
        "        #initial_epoch=9, \n",
        "        #steps_per_epoch=20,\n",
        "        callbacks=[\n",
        "            WandbCallback(log_batch_frequency=10, verbose=1),\n",
        "            GANMonitor(num_img=8, latent_dim=LATENT_DIM),\n",
        "            WandbLogger(model_name+experiment_name, run=run),\n",
        "            FIDLogger(real_embeddings, real_labels)\n",
        "            ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "12663/12663 [==============================] - 1099s 87ms/step - d_loss: 0.8448 - g_loss: 0.8189 - d_loss_realfake: 0.6906 - d_loss_attr: 0.2253 - g_loss_realfake: 0.8075 - g_loss_attr: 0.0974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 1.4s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "468ac7f2109d473e9f25ac7d47384d9d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "305.2555862259417\n",
            "Epoch 2/200\n",
            "12663/12663 [==============================] - 1098s 87ms/step - d_loss: 0.7889 - g_loss: 0.7646 - d_loss_realfake: 0.6999 - d_loss_attr: 0.1289 - g_loss_realfake: 0.7775 - g_loss_attr: 0.0254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 1.7s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4decd1a3377b45db8bc0158a6e402a58",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "231.73259498890428\n",
            "Epoch 3/200\n",
            "12663/12663 [==============================] - 1093s 86ms/step - d_loss: 0.7749 - g_loss: 0.7582 - d_loss_realfake: 0.6997 - d_loss_attr: 0.1095 - g_loss_realfake: 0.7680 - g_loss_attr: 0.0175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 1.7s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9ce18f0df3247cbbdd2623b41524e49",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "217.3493282709306\n",
            "Epoch 4/200\n",
            " 4966/12663 [==========>...................] - ETA: 11:02 - d_loss: 0.7696 - g_loss: 0.7566 - d_loss_realfake: 0.6993 - d_loss_attr: 0.1016 - g_loss_realfake: 0.7645 - g_loss_attr: 0.0146"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCtU1iUOww74"
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN)\n",
        "tf.compat.v1.logging.get_verbosity()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8cMrL0UT8o_"
      },
      "source": [
        "for i in range(19):\n",
        "  plt.imshow(gan.generator(tf.random.normal(shape=(1, LATENT_DIM)))[0])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUy0jgOv7FWE"
      },
      "source": [
        "# load model\n",
        "\n",
        "artifact = run.use_artifact('buio/GAN/gan_models:v1', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "run.join()\n",
        "\n",
        "print('\\n', artifact_dir)\n",
        "models = os.listdir(artifact_dir)\n",
        "generators = [model for model in models if 'generator' in model]\n",
        "discriminators = [model for model in models if 'discriminator' in model]\n",
        "\n",
        "def run_num(run_name):\n",
        "    return int(run_name.strip('h5').strip('.').split('_')[-1])\n",
        "\n",
        "generators.sort(key=run_num)\n",
        "discriminators.sort(key=run_num)\n",
        "print(*generators,sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQaCqC7dAHnl"
      },
      "source": [
        "# ri-populate model\n",
        "generator = tf.keras.models.load_model(artifact_dir+'/'+generators[-1])\n",
        "discriminator = tf.keras.models.load_model(artifact_dir+'/'+discriminators[-1])\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "gan.fit(dataset, epochs=1, steps_per_epoch=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgYeq3MGlNLn"
      },
      "source": [
        "# Losses\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clcRu8NkZpWK"
      },
      "source": [
        "# Example 1: (batch_size = 1, number of samples = 4)\n",
        "y_true = [0, 1, 0, 0]\n",
        "y_pred = [0.1, 0.9, 0.1, 0.1]\n",
        "bce = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.00)\n",
        "bce(y_true, y_pred).numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDkxf73naLuc"
      },
      "source": [
        "bce([0]*16,discriminator(next(iter(dataset)))[:,0]).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxsZj8XX5ghQ"
      },
      "source": [
        "Some of the last generated images around epoch 30\n",
        "(results keep improving after that):\n",
        "\n",
        "![results](https://i.imgur.com/h5MtQZ7l.png)"
      ]
    }
  ]
}