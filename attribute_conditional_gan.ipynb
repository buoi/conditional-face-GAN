{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attribute_conditional_gan",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5d76795b0ec431494e9853500d96f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c6a57ebec99a49609866f76619fe81d8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_30705ca4dcc44469b7e2b5ea0f5d71d3",
              "IPY_MODEL_2518168f4217428fbff631ce1f57dbb2",
              "IPY_MODEL_02cf7dc81a0445fa80e20d1f634ad828"
            ]
          }
        },
        "c6a57ebec99a49609866f76619fe81d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "30705ca4dcc44469b7e2b5ea0f5d71d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b8bd26ccd56e4c0fa4332683282acd59",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9545e53a0fea491e8b1fc81d317754f3"
          }
        },
        "2518168f4217428fbff631ce1f57dbb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_06f0beb281ea471faa1fe0b8e36c69de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 50,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e400e2f4aca4ec9bfd8833977bce304"
          }
        },
        "02cf7dc81a0445fa80e20d1f634ad828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f467281483674c5bb890d8df97bc9f6a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50/50 [01:17&lt;00:00,  1.37s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd9ee7ca5b0a4431a1298c2ec536609b"
          }
        },
        "b8bd26ccd56e4c0fa4332683282acd59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9545e53a0fea491e8b1fc81d317754f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06f0beb281ea471faa1fe0b8e36c69de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e400e2f4aca4ec9bfd8833977bce304": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f467281483674c5bb890d8df97bc9f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd9ee7ca5b0a4431a1298c2ec536609b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cd4464d5e3c44acae2b9eff48ee1e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cdb2c9b2fd9e4aacbb161802e9da2983",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0d9305e01f749b58ac904b116d810ba",
              "IPY_MODEL_beb2f93b18144f35bcc5cd3333b3f61c"
            ]
          }
        },
        "cdb2c9b2fd9e4aacbb161802e9da2983": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0d9305e01f749b58ac904b116d810ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_f5e50e9e5a1b465eb4d6ff50fadf8dae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 834.95MB of 834.95MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71d1dce1b4ab4c588d0c523dd72c15b5"
          }
        },
        "beb2f93b18144f35bcc5cd3333b3f61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_991f09e22e6d48fd94c0cd4299db450e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2be1fd30b1b44bca580c039c6b05771"
          }
        },
        "f5e50e9e5a1b465eb4d6ff50fadf8dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71d1dce1b4ab4c588d0c523dd72c15b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "991f09e22e6d48fd94c0cd4299db450e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2be1fd30b1b44bca580c039c6b05771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buoi/conditional-face-GAN/blob/main/attribute_conditional_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnwd03g75ghA"
      },
      "source": [
        "# Attribute Conditional Face Generation with GANs\n",
        "\n",
        "Marco Buiani  \n",
        "Matteo Folli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n352Hvd_GVP"
      },
      "source": [
        "# ⚙️ Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntbrzEpm_GVQ"
      },
      "source": [
        "## Fix random seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApxhNtGx_GVQ"
      },
      "source": [
        "SEED = 11\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW_uaYIi_GVQ"
      },
      "source": [
        "## Automatic Mixed Precision\n",
        "if supported by GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OldYCDQ0_GVQ",
        "outputId": "e1d3991c-b093-40a0-c942-a0ece6831ac9"
      },
      "source": [
        "_, gpu_name  = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "\n",
        "if gpu_name == 'Tesla T4':\n",
        "    from tensorflow.keras import mixed_precision\n",
        "\n",
        "    policy = mixed_precision.Policy('mixed_float16')\n",
        "    mixed_precision.set_global_policy(policy)\n",
        "\n",
        "!nvidia-smi -L"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-9ed3519e-ab2f-d305-0516-52058430afb1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4eR_-2j_GVQ"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1tGj6N7_GVR",
        "outputId": "9aeab37f-ab26-49c2-e7e1-de65acbb8168"
      },
      "source": [
        "from scipy import linalg\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "from tensorflow.keras.layers import Layer, Input, Dense, Reshape, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, ReLU, LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout, Embedding, Concatenate, Add, Activation\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, UpSampling2D, BatchNormalization\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.layers import SpectralNormalization\n",
        "\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL758kqufZWu"
      },
      "source": [
        "## Download CelebA\n",
        "\n",
        "We'll use face images from the CelebA dataset, resized to 64x64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40CS8jHvfZWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac0bb0c6-f659-4e60-e864-0e804d789b3b"
      },
      "source": [
        "try:\n",
        "    os.makedirs(\"celeba_gan\")\n",
        "    url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "    output = \"celeba_gan/img_align_celeba.zip\"\n",
        "    gdown.download(url, output, quiet=True)\n",
        "\n",
        "    with ZipFile(\"celeba_gan/img_align_celeba.zip\", \"r\") as zipobj:\n",
        "        zipobj.extractall(\"celeba_gan\")\n",
        "        \n",
        "except FileExistsError:\n",
        "    print(\"Dataset Already downloaded\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Already downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ32Qan0YOBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2320f766-8150-4e7f-ed1d-ab97f931d6c5"
      },
      "source": [
        "#Download labels from public github, they have been processed in a 0,1 csv file\n",
        "!wget -q -O \"/content/celeba_gan/list_attr_celeba01.csv.zip\" \"https://github.com/buoi/conditional-face-GAN/blob/main/list_attr_celeba01.csv.zip?raw=true\" \n",
        "!unzip -o \"/content/celeba_gan/list_attr_celeba01.csv.zip\" -d \"/content/celeba_gan\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/celeba_gan/list_attr_celeba01.csv.zip\n",
            "  inflating: /content/celeba_gan/list_attr_celeba01.csv  \n",
            "  inflating: /content/celeba_gan/__MACOSX/._list_attr_celeba01.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uR3O5gxul1x"
      },
      "source": [
        "## Dataset preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5D5SOGWhsNc"
      },
      "source": [
        "# image utils functions\n",
        "\n",
        "def conv_range(in_range=(-1,1), out_range=(0,255)):\n",
        "    \"\"\" Returns range conversion function\"\"\"\n",
        "\n",
        "    # compute means and spans once\n",
        "    in_mean, out_mean = np.mean(in_range), np.mean(out_range)\n",
        "    in_span, out_span = np.ptp(in_range), np.ptp(out_range)\n",
        "\n",
        "    # return function\n",
        "    def convert_img_range(in_img):\n",
        "        out_img = (in_img - in_mean) / in_span\n",
        "        out_img = out_img * out_span + out_mean\n",
        "        return out_img\n",
        "\n",
        "    return convert_img_range\n",
        "\n",
        "def crop128(img):\n",
        "    #return img[:, 77:141, 57:121]# 64,64 center crop\n",
        "    return img[:, 45:173, 25:153] # 128,128 center crop\n",
        "\n",
        "def resize64(img):\n",
        "    return tf.image.resize(img, (64,64), antialias=True, method='bilinear')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lguFA8Wg5ghJ"
      },
      "source": [
        "# 💾  Build Dataset \n",
        "create Dataset object from our folder, and rescale the images to the [-1,1] range:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "0WO2104NnsGh",
        "outputId": "cc340eb2-fe3b-4894-f4f6-c55bd15bc0f8"
      },
      "source": [
        "#@title Select Attributes {form-width: \"50%\", display-mode: \"form\" }\n",
        "\n",
        "NUMBER_OF_ATTRIBUTES = \"0\" #@param [0, 2, 10, 40]\n",
        "N_ATTRIBUTES = int(NUMBER_OF_ATTRIBUTES)\n",
        "\n",
        "IMAGE_SHAPE = '64x64x3' #@param ['64x64x3', '218x178x3']\n",
        "IMAGE_SHAPE = tuple(int(n) for n in IMAGE_SHAPE.split('x'))\n",
        "IMAGE_SIZE = IMAGE_SHAPE[0]\n",
        "\n",
        "BATCH_SIZE =  64 #@param {type: \"number\"}\n",
        "\n",
        "IMAGE_RANGE = '11'\n",
        "\n",
        "if N_ATTRIBUTES == 2:\n",
        "    LABELS = [\"Male\", \"Smiling\"]\n",
        "\n",
        "elif N_ATTRIBUTES == 10:\n",
        "    LABELS = [\n",
        "          \"Mouth_Slightly_Open\", \"Wearing_Lipstick\", \"High_Cheekbones\", \"Male\", \"Smiling\", \n",
        "          \"Heavy_Makeup\", \"Wavy_Hair\", \"Oval_Face\", \"Pointy_Nose\", \"Arched_Eyebrows\"]\n",
        "\n",
        "elif N_ATTRIBUTES == 40:\n",
        "    LABELS = [\n",
        "            '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n",
        "            'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n",
        "            'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows',\n",
        "            'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n",
        "            'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
        "            'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',\n",
        "            'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns',\n",
        "            'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n",
        "            'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',\n",
        "            'Wearing_Necktie', 'Young']\n",
        "\n",
        "else:\n",
        "    LABELS = [\"Male\", \"Smiling\"]# just for dataset creation\n",
        " \n",
        "\n",
        "# Take labels and a list of image locations in memory\n",
        "df = pd.read_csv(r\"/content/celeba_gan/list_attr_celeba01.csv\")\n",
        "im_list = df['image_id'].tolist()\n",
        "\n",
        "\n",
        "# load image at exact resolution\n",
        "dataset_train = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=\"int\", \n",
        "    labels = df[LABELS].values.tolist(),\n",
        "    image_size=(218, 178), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=SEED)\n",
        "\n",
        "\n",
        "def preprocess(dataset):\n",
        "    # extract the center crop\n",
        "    if IMAGE_SIZE == 64:\n",
        "        dataset = dataset.map(lambda x,y: (crop128(x),y) )\n",
        "        # resize\n",
        "        dataset = dataset.map(lambda x,y: (resize64(x),y) )\n",
        "    # convert image range\n",
        "    dataset = dataset.map(lambda x,y: (conv_range([0,255], [-1,1])(x),y) )\n",
        "    return dataset\n",
        "\n",
        "dataset_train = preprocess(dataset_train)\n",
        "\n",
        "if N_ATTRIBUTES !=0: \n",
        "    batch_img, batch_labels = next(iter(dataset_train))\n",
        "    plt.title('\\n'.join([(1-label)*'Not_'+LABELS[i]+' •'*int(label) for i,label in enumerate(batch_labels[0])]))\n",
        "\n",
        "else: # remove labels\n",
        "    dataset_train = dataset_train.map(lambda x,y: x)\n",
        "    batch_img = next(iter(dataset_train))\n",
        "    \n",
        "plt.imshow(batch_img[0].numpy()/2+0.5)\n",
        "\n",
        "print(\"image in range: \", np.min(batch_img), np.max(batch_img))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 202599 files belonging to 2 classes.\n",
            "image in range:  -1.0 1.0000004\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71hzB1gkkr9N"
      },
      "source": [
        "if N_ATTRIBUTES !=0:\n",
        "    attribute_cumsums = np.sum(df[LABELS].values, axis=0)\n",
        "\n",
        "    attribute_frequency = attribute_cumsums/df[LABELS].values.shape[0]\n",
        "    # shows how much each attribute deviates from being 50%\n",
        "    freq_sorted, labels_sorted  = zip(*sorted(zip(attribute_frequency, LABELS)))\n",
        "    freq_sorted = np.array(freq_sorted)\n",
        "\n",
        "    plt.figure(figsize=(8,10))\n",
        "    plt.yticks(np.arange(N_ATTRIBUTES),labels=labels_sorted, fontsize=10)\n",
        "    plt.axvline(x=0.5)\n",
        "    plt.barh(np.arange(N_ATTRIBUTES), freq_sorted);"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR88BN23cXLJ"
      },
      "source": [
        "# 🕵🏻‍♂️👩🏼‍🎨 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZDhzbt6otS_"
      },
      "source": [
        "## Custom Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK_8lLqo8C76"
      },
      "source": [
        "### Spectral Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umWst9YtrbQd"
      },
      "source": [
        "from tensorflow_addons.layers import SpectralNormalization"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22p6rrrorsK-"
      },
      "source": [
        "### Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3j_5bBurrHm"
      },
      "source": [
        "from keras.layers import InputSpec\n",
        "import keras.backend as K\n",
        "\n",
        "class SelfAttention(Layer):\n",
        "\n",
        "    def __init__(self, ch, **kwargs):\n",
        "        super(SelfAttention, self).__init__(**kwargs)\n",
        "        self.channels = ch\n",
        "        self.filters_f_g = self.channels // 8\n",
        "        self.filters_h = self.channels\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n",
        "        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n",
        "\n",
        "        # Create a trainable weight variable for this layer:\n",
        "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros', trainable=True)\n",
        "        self.kernel_f = self.add_weight(shape=kernel_shape_f_g,\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_f',\n",
        "                                        trainable=True)\n",
        "        self.kernel_g = self.add_weight(shape=kernel_shape_f_g,\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_g',\n",
        "                                        trainable=True)\n",
        "        self.kernel_h = self.add_weight(shape=kernel_shape_h,\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_h',\n",
        "                                        trainable=True)\n",
        "\n",
        "        super(SelfAttention, self).build(input_shape)\n",
        "        # Set input spec.\n",
        "        self.input_spec = InputSpec(ndim=4,\n",
        "                                    axes={3: input_shape[-1]})\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, x):\n",
        "        def hw_flatten(x):\n",
        "            return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[3]])\n",
        "\n",
        "        f = K.conv2d(x,\n",
        "                     kernel=self.kernel_f,\n",
        "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
        "        g = K.conv2d(x,\n",
        "                     kernel=self.kernel_g,\n",
        "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
        "        h = K.conv2d(x,\n",
        "                     kernel=self.kernel_h,\n",
        "                     strides=(1, 1), padding='same')  # [bs, h, w, c]\n",
        "\n",
        "        s = K.batch_dot(hw_flatten(g), K.permute_dimensions(hw_flatten(f), (0, 2, 1)))  # # [bs, N, N]\n",
        "\n",
        "        beta = K.softmax(s, axis=-1)  # attention map\n",
        "\n",
        "        o = K.batch_dot(beta, hw_flatten(h))  # [bs, N, C]\n",
        "\n",
        "        o = K.reshape(o, shape=K.shape(x))  # [bs, h, w, C]\n",
        "        x = self.gamma * o + x\n",
        "\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U0P_F--9fFv"
      },
      "source": [
        "### Minibatch Standard Deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqIs3VVhKVst"
      },
      "source": [
        "# mini-batch standard deviation layer\n",
        "class MinibatchStdev(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MinibatchStdev, self).__init__(**kwargs)\n",
        "\n",
        "    # calculate the mean standard deviation across each pixel coord\n",
        "    def call(self, inputs):\n",
        "        mean = K.mean(inputs, axis=0, keepdims=True)\n",
        "        mean_sq_diff = K.mean(K.square(inputs - mean), axis=0, keepdims=True) + 1e-8\n",
        "        mean_pix = K.mean(K.sqrt(mean_sq_diff), keepdims=True)\n",
        "        shape = K.shape(inputs)\n",
        "        output = K.tile(mean_pix, [shape[0], shape[1], shape[2], 1])\n",
        "        return K.concatenate([inputs, output], axis=-1)\n",
        "\n",
        "    # define the output shape of the layer\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_shape = list(input_shape)\n",
        "        input_shape[-1] += 1\n",
        "        return tuple(input_shape)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8teOYwQ7bWo"
      },
      "source": [
        "class MinibatchStdev(Layer):\n",
        "    # initialize the layer\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MinibatchStdev, self).__init__(**kwargs)\n",
        "\n",
        "    # perform the operation\n",
        "    def call(self, inputs):\n",
        "        # calculate the mean value for each pixel across channels\n",
        "        mean = tf.keras.backend.mean(inputs, axis=0, keepdims=True)\n",
        "        # calculate the squared differences between pixel values and mean\n",
        "        squ_diffs = tf.keras.backend.square(inputs - mean)\n",
        "        # calculate the average of the squared differences (variance)\n",
        "        mean_sq_diff = tf.keras.backend.mean(squ_diffs, axis=0, keepdims=True)\n",
        "        # add a small value to avoid a blow-up when we calculate stdev\n",
        "        mean_sq_diff += 1e-8\n",
        "        # square root of the variance (stdev)\n",
        "        stdev = tf.keras.backend.sqrt(mean_sq_diff)\n",
        "        # calculate the mean standard deviation across each pixel coord\n",
        "        mean_pix = tf.keras.backend.mean(stdev, keepdims=True)\n",
        "        # scale this up to be the size of one input feature map for each sample\n",
        "        shape = tf.keras.backend.shape(inputs)\n",
        "        output = tf.keras.backend.tile(mean_pix, (shape[0], shape[1], shape[2], 1))\n",
        "        # concatenate with the output\n",
        "        combined = tf.keras.backend.concatenate([inputs, output], axis=-1)\n",
        "        return combined\n",
        "\n",
        "    # define the output shape of the layer\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # create a copy of the input shape as a list\n",
        "        input_shape = list(input_shape)\n",
        "        # add one to the channel dimension (assume channels-last)\n",
        "        input_shape[-1] += 1\n",
        "        # convert list to a tuple\n",
        "        return tuple(input_shape)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTEy14860sfy"
      },
      "source": [
        "class PixelNormalization(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(PixelNormalization, self).__init__(**kwargs)\n",
        "        self.epsilon= 1e-8\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        #return inputs * tf.math.rsqrt(tf.reduce_mean(tf.math.square(inputs), axis=1, keepdims=True) + self.epsilon)\n",
        "        # Calculate square pixel values\n",
        "        values = inputs**2.0\n",
        "        # Calculate the mean pixel values\n",
        "        mean_values = tf.keras.backend.mean(values, axis=-1, keepdims=True)\n",
        "        # Ensure the mean is not zero\n",
        "        mean_values += 1.0e-8\n",
        "        # Calculate the sqrt of the mean squared value (L2 norm)\n",
        "        l2 = tf.keras.backend.sqrt(mean_values)\n",
        "        # Normalize values by the l2 norm\n",
        "        normalized = inputs / l2\n",
        "        return normalized\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EOUGrgT9iZp"
      },
      "source": [
        "## Define Building Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYUDMhZ85KPv"
      },
      "source": [
        "### BN stddev 218x178"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0btKrPq5KPw",
        "outputId": "92b70f78-2a17-4142-ad17-ba885543ef1d"
      },
      "source": [
        "# revisited\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "def define_generator218(latent_dim = 100, n_attributes=N_ATTRIBUTES):\n",
        "#\n",
        "    \"\"\"\n",
        "    carpedm20's generator is quasi-identical to soumith's\n",
        "    except at the 1st layer: carpedm20's was easier, just\n",
        "    a fully connected layer and a reshape.\n",
        "    \"\"\"\n",
        "\n",
        "    n_channels_large = 128\n",
        "    #n_channels_large = 64\n",
        "    z = keras.Input(shape=(latent_dim,))\n",
        "\n",
        "\n",
        "    #z_attr = layers.Concatenate(axis=-1)([z, tf.cast(in_attr, dtype='float32')])\n",
        "\n",
        "    first_plane_shape = (7, 6, n_channels_large*8)\n",
        "\n",
        "    gen_img = layers.Dense(np.prod(first_plane_shape),\n",
        "                          kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                          bias_initializer='zeros',)(z)\n",
        "\n",
        "    gen_img = layers.Reshape(first_plane_shape)(gen_img)\n",
        "    gen_img = layers.Activation('relu')(gen_img)\n",
        "\n",
        "    #  attr input, dense,       reshape\\\n",
        "    #latent input, dense, relu, reshape/ concat, conv, BN\n",
        "    if N_ATTRIBUTES !=0:\n",
        "        in_attr = layers.Input(shape=(n_attributes))\n",
        "        # attr -> 4*4\n",
        "        attr_n_maps = {2:1, 10:1, 40:3}[N_ATTRIBUTES]\n",
        "        attr_map = layers.Dense(np.prod(first_plane_shape[0]* first_plane_shape[1]*attr_n_maps),\n",
        "                        kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                        bias_initializer='zeros',)(in_attr)\n",
        "                        \n",
        "        attr_map = layers.Reshape((first_plane_shape[0], first_plane_shape[1], attr_n_maps))(attr_map)\n",
        "        attr_map = layers.Activation('relu')(attr_map)\n",
        "\n",
        "\n",
        "        #gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)    epsilon=1e-5, scale=True)(gen_img)\n",
        "\n",
        "        #(None, 4,4, 1024) (None, 4,4,1)\n",
        "        gen_img = layers.Concatenate()((gen_img, attr_map))\n",
        "\n",
        "\n",
        "    # plane shape: (8,8,128*4)\n",
        "    gen_img = SpectralNormalization(\n",
        "        layers.Conv2DTranspose(filters=n_channels_large*4, kernel_size=5,\n",
        "                      kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(gen_img)\n",
        "\n",
        "    gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    #gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    gen_img = layers.Activation('relu')(gen_img)\n",
        "\n",
        "\n",
        "    # plane shape: (16,16,128*2)\n",
        "    gen_img = SpectralNormalization(\n",
        "        layers.Conv2DTranspose(filters=n_channels_large*2, kernel_size=5,\n",
        "                      kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(gen_img)\n",
        "    gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    #gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    gen_img = layers.Activation('relu')(gen_img)\n",
        "\n",
        "    ###gen_img = SelfAttention(n_channels_large*2)(gen_img)\n",
        "\n",
        "\n",
        "\n",
        "    # plane shape: (32,32,128)\n",
        "    gen_img = SpectralNormalization(\n",
        "        layers.Conv2DTranspose(filters=n_channels_large, kernel_size=5,\n",
        "                      kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(gen_img)\n",
        "    gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    #gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    gen_img = layers.Activation('relu')(gen_img)\n",
        "\n",
        "    gen_img = tf.keras.layers.Cropping2D((0,1))(gen_img)\n",
        "\n",
        "\n",
        "    gen_img = SpectralNormalization(\n",
        "        layers.Conv2DTranspose(filters=n_channels_large, kernel_size=5,\n",
        "                      kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(gen_img)\n",
        "    gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    #gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    gen_img = layers.Activation('relu')(gen_img)\n",
        "\n",
        "\n",
        "    # plane shape: (64,64,3)\n",
        "    gen_img = Conv2DTranspose(filters=3, kernel_size=5,\n",
        "                      kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same')(gen_img)\n",
        "    \n",
        "    gen_img = tf.keras.layers.Cropping2D((3,3))(gen_img)\n",
        "\n",
        "    gen_img = layers.Activation(activation='tanh', dtype='float32')(gen_img)\n",
        "\n",
        "    if N_ATTRIBUTES !=0:\n",
        "        generator = keras.Model(inputs= [z, in_attr], outputs=gen_img, name='cond_gen')\n",
        "    else:\n",
        "        generator = keras.Model(inputs=z, outputs=gen_img, name='gen')\n",
        "    return generator\n",
        "    return generator\n",
        "\n",
        "def define_discriminator218(n_attributes=N_ATTRIBUTES):\n",
        "    \"\"\"\n",
        "    carpedm20's discriminator is quasi-identical to soumith's\n",
        "    except at the 1st layer: carpedm20's was easier, just\n",
        "    the final layer being diff.\n",
        "    \"\"\"\n",
        "\n",
        "    ndf = 128  # ndf or NDF: Number of Discriminator Filters\n",
        "\n",
        "    input_img = layers.Input(shape=(218,178,3))\n",
        "\n",
        "    #layers.RandomFlip(mode='horizontal')(input_img)\n",
        "\n",
        "    # plane shape: (32,32,ndf)\n",
        "    d_output_proba = SpectralNormalization(\n",
        "        layers.Conv2D(filters=ndf, kernel_size=5,\n",
        "                      kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(input_img)\n",
        "\n",
        "    d_output_proba = layers.LeakyReLU(alpha=0.2)(d_output_proba)\n",
        "\n",
        "    d_output_proba = SpectralNormalization(\n",
        "        layers.Conv2D(filters=ndf*2, kernel_size=5,\n",
        "                      kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(d_output_proba)\n",
        "    d_output_proba = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(d_output_proba)\n",
        "    d_output_proba = layers.LeakyReLU(alpha=0.2)(d_output_proba)\n",
        "\n",
        "    # plane shape: (16,16,ndf*2)\n",
        "    d_output_proba = SpectralNormalization(\n",
        "        layers.Conv2D(filters=ndf*2, kernel_size=5,\n",
        "                      kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(d_output_proba)\n",
        "    d_output_proba = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(d_output_proba)\n",
        "    d_output_proba = layers.LeakyReLU(alpha=0.2)(d_output_proba)\n",
        "\n",
        "    ##d_output_proba = SelfAttention(ndf*2)(d_output_proba)\n",
        "\n",
        "    # plane shape: (8,8,ndf*4)\n",
        "    d_output_proba = SpectralNormalization(\n",
        "        layers.Conv2D(filters=ndf*4, kernel_size=5,\n",
        "                      kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(d_output_proba)\n",
        "    d_output_proba = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(d_output_proba)\n",
        "    d_output_proba = layers.LeakyReLU(alpha=0.2)(d_output_proba)\n",
        "    \n",
        "\n",
        "    # plane shape: (4,4,ndf*8)\n",
        "    d_output_proba = Conv2D(filters=ndf*8, kernel_size=5,\n",
        "                      kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same')(d_output_proba)\n",
        "                      \n",
        "\n",
        "    d_output_proba = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(d_output_proba)\n",
        "    #\n",
        "    d_output_proba = MinibatchStdev()(d_output_proba)\n",
        "    #\n",
        "    d_output_proba = layers.LeakyReLU(alpha=0.2)(d_output_proba)\n",
        "\n",
        "    \"\"\" Canceled and replaced by what follows it.\n",
        "    # plane shape: (1,1,1)\n",
        "    d_output_proba = layers.Conv2D(filters=1, kernel_size=5,\n",
        "                                  strides=1, padding='valid',\n",
        "                                  activation='sigmoid')(d_output_proba)\n",
        "    d_output_proba = layers.Reshape((1,))(d_output_proba)\n",
        "    \"\"\"\n",
        "\n",
        "    # Flatten and Dense to (None, 1) \\in [0, 1].\n",
        "    d_output_proba = layers.Flatten()(d_output_proba)\n",
        "\n",
        "    out_realfake = layers.Dense(1,kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                          bias_initializer='zeros',)(d_output_proba)\n",
        "\n",
        "    out_realfake = layers.Activation('sigmoid', name='sigmoid_realfake', dtype='float32')(out_realfake)\n",
        "\n",
        "    if N_ATTRIBUTES !=0:\n",
        "        out_attr = layers.Dense(n_attributes, dtype='float32')(d_output_proba)\n",
        "        out_attr = layers.Activation('sigmoid', name='sigmoid_label', dtype='float32')(out_attr)\n",
        "        discriminator = keras.Model(inputs=input_img, outputs=[out_realfake, out_attr], name='cond_discr')\n",
        "    else:\n",
        "        discriminator = keras.Model(inputs=input_img, outputs=out_realfake, name='discr')\n",
        "\n",
        "    return discriminator\n",
        "\n",
        "\n",
        "\n",
        "define_generator218().summary()\n",
        "define_discriminator218().summary()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"gen\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 43008)             4343808   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 7, 6, 1024)        0         \n",
            "_________________________________________________________________\n",
            "activation_94 (Activation)   (None, 7, 6, 1024)        0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization (Spec (None, 14, 12, 512)       13108736  \n",
            "_________________________________________________________________\n",
            "batch_normalization_94 (Batc (None, 14, 12, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_95 (Activation)   (None, 14, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_1 (Sp (None, 28, 24, 256)       3277568   \n",
            "_________________________________________________________________\n",
            "batch_normalization_95 (Batc (None, 28, 24, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_96 (Activation)   (None, 28, 24, 256)       0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_2 (Sp (None, 56, 48, 128)       819584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_96 (Batc (None, 56, 48, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_97 (Activation)   (None, 56, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "cropping2d (Cropping2D)      (None, 56, 46, 128)       0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_3 (Sp (None, 112, 92, 128)      409856    \n",
            "_________________________________________________________________\n",
            "batch_normalization_97 (Batc (None, 112, 92, 128)      512       \n",
            "_________________________________________________________________\n",
            "activation_98 (Activation)   (None, 112, 92, 128)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 224, 184, 3)       9603      \n",
            "_________________________________________________________________\n",
            "cropping2d_1 (Cropping2D)    (None, 218, 178, 3)       0         \n",
            "_________________________________________________________________\n",
            "activation_99 (Activation)   (None, 218, 178, 3)       0         \n",
            "=================================================================\n",
            "Total params: 21,973,251\n",
            "Trainable params: 21,969,283\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n",
            "Model: \"discr\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 218, 178, 3)]     0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_4 (Sp (None, 109, 89, 128)      9856      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 109, 89, 128)      0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_5 (Sp (None, 55, 45, 256)       819712    \n",
            "_________________________________________________________________\n",
            "batch_normalization_98 (Batc (None, 55, 45, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 55, 45, 256)       0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_6 (Sp (None, 28, 23, 256)       1638912   \n",
            "_________________________________________________________________\n",
            "batch_normalization_99 (Batc (None, 28, 23, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 28, 23, 256)       0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_7 (Sp (None, 14, 12, 512)       3277824   \n",
            "_________________________________________________________________\n",
            "batch_normalization_100 (Bat (None, 14, 12, 512)       2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 14, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_98 (Conv2D)           (None, 7, 6, 1024)        13108224  \n",
            "_________________________________________________________________\n",
            "batch_normalization_101 (Bat (None, 7, 6, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "minibatch_stdev (MinibatchSt (None, 7, 6, 1025)        0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 7, 6, 1025)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 43050)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 43051     \n",
            "_________________________________________________________________\n",
            "sigmoid_realfake (Activation (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 18,905,771\n",
            "Trainable params: 18,900,523\n",
            "Non-trainable params: 5,248\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csKGBARmNimn"
      },
      "source": [
        "### BN stddev 64x64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oXSHQOpPRRr",
        "outputId": "dadc2c3a-7b23-4fb1-cd74-28f5ffff6ada"
      },
      "source": [
        "# revisited\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "def define_generator64(latent_dim = 100, n_attributes=N_ATTRIBUTES):\n",
        "#\n",
        "    \"\"\"\n",
        "    carpedm20's generator is quasi-identical to soumith's\n",
        "    except at the 1st layer: carpedm20's was easier, just\n",
        "    a fully connected layer and a reshape.\n",
        "    \"\"\"\n",
        "\n",
        "    n_channels_large = 128\n",
        "    #n_channels_large = 64\n",
        "\n",
        "    z = keras.Input(shape=(latent_dim,))\n",
        "\n",
        "    #z_attr = layers.Concatenate(axis=-1)([z, tf.cast(in_attr, dtype='float32')])\n",
        "\n",
        "    first_plane_shape = (4, 4, n_channels_large*8)\n",
        "\n",
        "    gen_img = layers.Dense(np.prod(first_plane_shape),\n",
        "                          kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                          bias_initializer='zeros',)(z)\n",
        "\n",
        "    gen_img = layers.Reshape(first_plane_shape)(gen_img)\n",
        "    gen_img = layers.Activation('relu')(gen_img)\n",
        "\n",
        "    #  attr input, dense,       reshape\\\n",
        "    #latent input, dense, relu, reshape/ concat, conv, BN\n",
        "    \n",
        "    if N_ATTRIBUTES !=0:\n",
        "        in_attr = layers.Input(shape=(n_attributes))\n",
        "        # attr -> 4*4\n",
        "        attr_n_maps = {2:1, 10:1, 40:3}[N_ATTRIBUTES]\n",
        "        attr_map = layers.Dense(np.prod(first_plane_shape[0]* first_plane_shape[1]*attr_n_maps),\n",
        "                        kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                        bias_initializer='zeros',)(in_attr)\n",
        "                        \n",
        "        attr_map = layers.Reshape((first_plane_shape[0], first_plane_shape[1], attr_n_maps))(attr_map)\n",
        "        attr_map = layers.Activation('relu')(attr_map)\n",
        "\n",
        "\n",
        "        #gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)    epsilon=1e-5, scale=True)(gen_img)\n",
        "\n",
        "        #(None, 4,4, 1024) (None, 4,4,1)\n",
        "        gen_img = layers.Concatenate()((gen_img, attr_map))\n",
        "\n",
        "\n",
        "    # plane shape: (8,8,128*4)\n",
        "    gen_img = SpectralNormalization(\n",
        "        layers.Conv2DTranspose(filters=n_channels_large*4, kernel_size=5,\n",
        "                      kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(gen_img)\n",
        "\n",
        "    gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    #gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    gen_img = layers.Activation('relu')(gen_img)\n",
        "\n",
        "\n",
        "    # plane shape: (16,16,128*2)\n",
        "    gen_img = SpectralNormalization(\n",
        "        layers.Conv2DTranspose(filters=n_channels_large*2, kernel_size=5,\n",
        "                      kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(gen_img)\n",
        "    gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    #gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    gen_img = layers.Activation('relu')(gen_img)\n",
        "\n",
        "    ###gen_img = SelfAttention(n_channels_large*2)(gen_img)\n",
        "\n",
        "\n",
        "\n",
        "    # plane shape: (32,32,128)\n",
        "    gen_img = SpectralNormalization(\n",
        "        layers.Conv2DTranspose(filters=n_channels_large, kernel_size=5,\n",
        "                      kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(gen_img)\n",
        "    gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    #gen_img = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(gen_img)\n",
        "    gen_img = layers.Activation('relu')(gen_img)\n",
        "\n",
        "\n",
        "\n",
        "    # plane shape: (64,64,3)\n",
        "    gen_img = Conv2DTranspose(filters=3, kernel_size=5,\n",
        "                      kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same')(gen_img)\n",
        "\n",
        "    gen_img = layers.Activation(activation='tanh', dtype='float32')(gen_img)\n",
        "\n",
        "    if N_ATTRIBUTES !=0:\n",
        "        generator = keras.Model(inputs= [z, in_attr], outputs=gen_img, name='cond_gen')\n",
        "    else:\n",
        "        generator = keras.Model(inputs=z, outputs=gen_img, name='gen')\n",
        "    return generator\n",
        "\n",
        "def define_discriminator64(n_attributes=N_ATTRIBUTES):\n",
        "    \"\"\"\n",
        "    carpedm20's discriminator is quasi-identical to soumith's\n",
        "    except at the 1st layer: carpedm20's was easier, just\n",
        "    the final layer being diff.\n",
        "    \"\"\"\n",
        "\n",
        "    ndf = 128  # ndf or NDF: Number of Discriminator Filters\n",
        "\n",
        "    input_img = layers.Input(shape=(64,64,3))\n",
        "\n",
        "    #layers.RandomFlip(mode='horizontal')(input_img)\n",
        "\n",
        "    # plane shape: (32,32,ndf)\n",
        "    d_output_proba = SpectralNormalization(\n",
        "        layers.Conv2D(filters=ndf, kernel_size=5,\n",
        "                      kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(input_img)\n",
        "\n",
        "    d_output_proba = layers.LeakyReLU(alpha=0.2)(d_output_proba)\n",
        "\n",
        "\n",
        "    # plane shape: (16,16,ndf*2)\n",
        "    d_output_proba = SpectralNormalization(\n",
        "        layers.Conv2D(filters=ndf*2, kernel_size=5,\n",
        "                      kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(d_output_proba)\n",
        "    d_output_proba = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(d_output_proba)\n",
        "    d_output_proba = layers.LeakyReLU(alpha=0.2)(d_output_proba)\n",
        "\n",
        "    ##d_output_proba = SelfAttention(ndf*2)(d_output_proba)\n",
        "\n",
        "    # plane shape: (8,8,ndf*4)\n",
        "    d_output_proba = SpectralNormalization(\n",
        "        layers.Conv2D(filters=ndf*4, kernel_size=5,\n",
        "                      kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same'), dtype='float32')(d_output_proba)\n",
        "    d_output_proba = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(d_output_proba)\n",
        "    d_output_proba = layers.LeakyReLU(alpha=0.2)(d_output_proba)\n",
        "\n",
        "    # plane shape: (4,4,ndf*8)\n",
        "    d_output_proba = Conv2D(filters=ndf*8, kernel_size=5,\n",
        "                      kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02,),\n",
        "                      strides=2, padding='same')(d_output_proba)\n",
        "\n",
        "    d_output_proba = layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(d_output_proba)\n",
        "    #\n",
        "    d_output_proba = MinibatchStdev()(d_output_proba)\n",
        "    #\n",
        "    d_output_proba = layers.LeakyReLU(alpha=0.2)(d_output_proba)\n",
        "\n",
        "    \"\"\" Canceled and replaced by what follows it.\n",
        "    # plane shape: (1,1,1)\n",
        "    d_output_proba = layers.Conv2D(filters=1, kernel_size=5,\n",
        "                                  strides=1, padding='valid',\n",
        "                                  activation='sigmoid')(d_output_proba)\n",
        "    d_output_proba = layers.Reshape((1,))(d_output_proba)\n",
        "    \"\"\"\n",
        "\n",
        "    # Flatten and Dense to (None, 1) \\in [0, 1].\n",
        "    d_output_proba = layers.Flatten()(d_output_proba)\n",
        "    #d_output_proba = GlobalAveragePooling2D()(d_output_proba)\n",
        "\n",
        "\n",
        "    out_realfake = layers.Dense(1,kernel_initializer=RandomNormal(stddev=0.02,),\n",
        "                          bias_initializer='zeros',)(d_output_proba)\n",
        "\n",
        "    out_realfake = layers.Activation('sigmoid', name='sigmoid_realfake', dtype='float32')(out_realfake)\n",
        "\n",
        "    if N_ATTRIBUTES !=0:\n",
        "        out_attr = layers.Dense(n_attributes, dtype='float32')(d_output_proba)\n",
        "        out_attr = layers.Activation('sigmoid', name='sigmoid_label', dtype='float32')(out_attr)\n",
        "        discriminator = keras.Model(inputs=input_img, outputs=[out_realfake, out_attr], name='cond_discr')\n",
        "    else:\n",
        "        discriminator = keras.Model(inputs=input_img, outputs=out_realfake, name='discr')\n",
        "\n",
        "    return discriminator\n",
        "\n",
        "\n",
        "define_generator64().summary()\n",
        "define_discriminator64().summary()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"gen\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16384)             1654784   \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "activation_100 (Activation)  (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_8 (Sp (None, 8, 8, 512)         13108736  \n",
            "_________________________________________________________________\n",
            "batch_normalization_102 (Bat (None, 8, 8, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_101 (Activation)  (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_9 (Sp (None, 16, 16, 256)       3277568   \n",
            "_________________________________________________________________\n",
            "batch_normalization_103 (Bat (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_102 (Activation)  (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_10 (S (None, 32, 32, 128)       819584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_104 (Bat (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_103 (Activation)  (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 64, 64, 3)         9603      \n",
            "_________________________________________________________________\n",
            "activation_104 (Activation)  (None, 64, 64, 3)         0         \n",
            "=================================================================\n",
            "Total params: 18,873,859\n",
            "Trainable params: 18,870,275\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "Model: \"discr\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_11 (S (None, 32, 32, 128)       9856      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_12 (S (None, 16, 16, 256)       819712    \n",
            "_________________________________________________________________\n",
            "batch_normalization_105 (Bat (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "spectral_normalization_13 (S (None, 8, 8, 512)         3277824   \n",
            "_________________________________________________________________\n",
            "batch_normalization_106 (Bat (None, 8, 8, 512)         2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_102 (Conv2D)          (None, 4, 4, 1024)        13108224  \n",
            "_________________________________________________________________\n",
            "batch_normalization_107 (Bat (None, 4, 4, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "minibatch_stdev_1 (Minibatch (None, 4, 4, 1025)        0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 1025)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 16400)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 16401     \n",
            "_________________________________________________________________\n",
            "sigmoid_realfake (Activation (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 17,239,185\n",
            "Trainable params: 17,234,705\n",
            "Non-trainable params: 4,480\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miESm-gD5ghL"
      },
      "source": [
        "## Define GAN Class\n",
        "override `train_step`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OD2mErvoefz"
      },
      "source": [
        "### AC-GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kZ3QK4Yoctb"
      },
      "source": [
        "class ACGAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim, n_attributes, **kwargs):\n",
        "        super(ACGAN, self).__init__(**kwargs)\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_attributes = n_attributes# deprecated, take from data\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn, loss_attr):\n",
        "        super(ACGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "        self.d_realfake_metric = keras.metrics.Mean(name=\"d_realfake\")\n",
        "        self.d_attr_metric = keras.metrics.Mean(name=\"d_attr\")\n",
        "\n",
        "        self.g_realfake_metric = keras.metrics.Mean(name=\"g_realfake\")\n",
        "        self.g_attr_metric = keras.metrics.Mean(name=\"g_attr\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        real_images, real_attr = data \n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        #n_attributes = real_attr.shape[-1]###### \n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        #random_attr = tf.random.uniform(shape=(batch_size, self.n_attributes), maxval=2, dtype='int64')\n",
        "\n",
        "        # Decode them to fake images\n",
        "        #generated_images = self.generator([random_latent_vectors, random_attr])\n",
        "        # using same real_attr instead of random sampled ones.. is this safe? #########\n",
        "        generated_images = self.generator([random_latent_vectors, real_attr])\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating fake(1) from real(0) images\n",
        "\n",
        "        labels_fake = tf.ones((batch_size, 1)) - 0.05 * tf.random.uniform((batch_size, 1))\n",
        "        labels_real = tf.zeros((batch_size, 1)) + 0.05 * tf.random.uniform((batch_size, 1))\n",
        "\n",
        "        labels_realfake = tf.concat([labels_fake, labels_real], axis=0)\n",
        "\n",
        "        \"\"\"labels_realfake = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
        "\n",
        "        labels_realfake = labels_realfake + 0.05 * tf.random.uniform(tf.shape(labels_realfake))\n",
        "        \"\"\"\n",
        "        #labels_attr = tf.concat([random_attr, real_attr], axis=0)\n",
        "        # NON HA SENSO LA LOSS SULLA SECONDA PARTE... OPPURE VA INVERTITA IN BASSO\n",
        "\n",
        "        labels_attr = tf.concat([real_attr, real_attr], axis=0) #????\n",
        "        labels_attr = tf.cast(labels_attr, dtype='float32')\n",
        "\n",
        "        # Add random noise to the labels - important trick!\n",
        "        #labels_attr = labels_attr + 0.05 * tf.random.uniform(tf.shape(labels_attr))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_realfake, pred_attr = self.discriminator(combined_images)\n",
        "            pred_real_attr = pred_attr[batch_size: 2*batch_size]\n",
        "\n",
        "            d_loss_realfake = self.loss_fn(labels_realfake, pred_realfake)\n",
        "            #print(f\"loss_fn({labels_realfake}, {pred_realfake})\")\n",
        "            #print('= ',d_loss_realfake)\n",
        "\n",
        "            d_loss_attr = self.loss_fn(real_attr, pred_real_attr)#######-1tf.reduce_mean(\n",
        "            #print(f\"loss_fn({labels_attr}, {pred_attr})\")\n",
        "            #print('= ',self.loss_fn(labels_attr, pred_attr))\n",
        "            #print('reduce_mean -> ',d_loss_attr)\n",
        "\n",
        "            #print(f\"d_loss_realfake: {d_loss_realfake} + d_loss_attr: {d_loss_attr}\")\n",
        "            d_loss = d_loss_realfake + d_loss_attr \n",
        "      \n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        ## GEN,\n",
        "        # Sample 2*batch_size so Gen sees the same number of images as Disc\n",
        "\n",
        "        # Sample NEW random points in the latent space (D has just optimized for previous ones)\n",
        "        random_latent_vectors = tf.random.normal(shape=(2*batch_size, self.latent_dim))\n",
        "        # [2*16, 128], [16,2]\n",
        "        #random_attr = tf.random.uniform(shape=(batch_size, self.n_attributes), maxval=2, dtype='int64')\n",
        "\n",
        "        # using sampled real attributes during entire training step\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((2*batch_size, 1)) # same as inverting loss sign?\n",
        "\n",
        "        ##### SHOULD WE SWITCH ATTRIBUTES?!\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_realfake, pred_attr = self.discriminator(\n",
        "                self.generator([random_latent_vectors, labels_attr]))\n",
        "            \n",
        "            g_loss_realfake = self.loss_fn(misleading_labels, pred_realfake)\n",
        "\n",
        "            #print(f\"loss_fn({misleading_labels}, {pred_realfake})\")\n",
        "            #print('= ',g_loss_realfake)\n",
        "\n",
        "            g_loss_attr = self.loss_fn(labels_attr, pred_attr) ##########tf.reduce_mean(\n",
        "            #print(f\"loss_fn({random_attr}, {pred_attr})\")\n",
        "            #print('= ',self.loss_fn(real_attr, pred_attr))\n",
        "            #print('reduce_mean -> ',g_loss_attr)\n",
        "\n",
        "            #print(f\"g_loss_realfake: {g_loss_realfake} + g_loss_attr: {g_loss_attr}\")\n",
        "            g_loss = g_loss_realfake + g_loss_attr\n",
        "\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "\n",
        "        self.d_realfake_metric.update_state(d_loss_realfake)\n",
        "        self.d_attr_metric.update_state(d_loss_attr)\n",
        "\n",
        "        self.g_realfake_metric.update_state(g_loss_realfake)\n",
        "        self.g_attr_metric.update_state(g_loss_attr)\n",
        "       \n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "\n",
        "            \"d_loss_realfake\": self.d_realfake_metric.result(),\n",
        "            \"d_loss_attr\": self.d_attr_metric.result(),\n",
        "            \"g_loss_realfake\": self.g_realfake_metric.result(),\n",
        "            \"g_loss_attr\": self.g_attr_metric.result()\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        latent, label = inputs\n",
        "        generated_images = self.generator([latent, label])\n",
        "        d_loss = self.discriminator(generated_images)\n",
        "        return generated_images, d_loss\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPs2Uor1Zsbr"
      },
      "source": [
        "### GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5ugvni_8Eqe"
      },
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim, **kwargs):\n",
        "        super(GAN, self).__init__(**kwargs)\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        generator_gain = 2\n",
        "        random_latent_vectors = tf.random.normal(shape=(generator_gain*batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((generator_gain*batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \n",
        "        generated_images = self.generator(inputs)\n",
        "        d_loss = self.discriminator(generated_images)\n",
        "        return generated_images, d_loss\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQdXIfxBfgFW"
      },
      "source": [
        "class WGAN(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        discriminator,\n",
        "        generator,\n",
        "        latent_dim,\n",
        "        discriminator_extra_steps=3,\n",
        "        gp_weight=10.0,\n",
        "    ):\n",
        "        super(WGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_steps = discriminator_extra_steps\n",
        "        self.gp_weight = gp_weight\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
        "        super(WGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_loss_fn = d_loss_fn\n",
        "        self.g_loss_fn = g_loss_fn\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "        \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "        This loss is calculated on an interpolated image\n",
        "        and added to the discriminator loss.\n",
        "        \"\"\"\n",
        "        # Get the interpolated image\n",
        "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "        diff = fake_images - real_images\n",
        "        interpolated = real_images + alpha * diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            # 1. Get the discriminator output for this interpolated image.\n",
        "            pred = self.discriminator(interpolated, training=True)\n",
        "\n",
        "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        # 3. Calculate the norm of the gradients.\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        return gp\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "\n",
        "        # Get the batch size\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        # For each batch, we are going to perform the\n",
        "        # following steps as laid out in the original paper:\n",
        "        # 1. Train the generator and get the generator loss\n",
        "        # 2. Train the discriminator and get the discriminator loss\n",
        "        # 3. Calculate the gradient penalty\n",
        "        # 4. Multiply this gradient penalty with a constant weight factor\n",
        "        # 5. Add the gradient penalty to the discriminator loss\n",
        "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
        "\n",
        "        # Train the discriminator first. The original paper recommends training\n",
        "        # the discriminator for `x` more steps (typically 5) as compared to\n",
        "        # one step of the generator. Here we will train it for 3 extra steps\n",
        "        # as compared to 5 to reduce the training time.\n",
        "        for i in range(self.d_steps):\n",
        "            # Get the latent vector\n",
        "            random_latent_vectors = tf.random.normal(\n",
        "                shape=(batch_size, self.latent_dim)\n",
        "            )\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Generate fake images from the latent vector\n",
        "                fake_images = self.generator(random_latent_vectors, training=True)\n",
        "                # Get the logits for the fake images\n",
        "                fake_logits = self.discriminator(fake_images, training=True)\n",
        "                # Get the logits for the real images\n",
        "                real_logits = self.discriminator(real_images, training=True)\n",
        "\n",
        "                # Calculate the discriminator loss using the fake and real image logits\n",
        "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
        "                # Calculate the gradient penalty\n",
        "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
        "                # Add the gradient penalty to the original discriminator loss\n",
        "                d_loss = d_cost + gp * self.gp_weight\n",
        "\n",
        "            # Get the gradients w.r.t the discriminator loss\n",
        "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "            # Update the weights of the discriminator using the discriminator optimizer\n",
        "            self.d_optimizer.apply_gradients(\n",
        "                zip(d_gradient, self.discriminator.trainable_variables)\n",
        "            )\n",
        "\n",
        "        # Train the generator\n",
        "        # Get the latent vector\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Generate fake images using the generator\n",
        "            generated_images = self.generator(random_latent_vectors, training=True)\n",
        "            # Get the discriminator logits for fake images\n",
        "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
        "            # Calculate the generator loss\n",
        "            g_loss = self.g_loss_fn(gen_img_logits)\n",
        "\n",
        "        # Get the gradients w.r.t the generator loss\n",
        "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        # Update the weights of the generator using the generator optimizer\n",
        "        self.g_optimizer.apply_gradients(\n",
        "            zip(gen_gradient, self.generator.trainable_variables)\n",
        "        )\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
        "        \n",
        "    def call(self, inputs):\n",
        "      \n",
        "        generated_images = self.generator(inputs)\n",
        "        d_loss = self.discriminator(generated_images)\n",
        "        return generated_images, d_loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hpZ9Cju5ghM"
      },
      "source": [
        "# 📉 Training callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSMPF5ELnBIv"
      },
      "source": [
        "### Images logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpWnnjZ7a_MN"
      },
      "source": [
        "class ImagesLogger(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=8, latent_dim=128, **kwargs):\n",
        "        super(ImagesLogger, self).__init__(**kwargs)\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "\n",
        "\n",
        "        if N_ATTRIBUTES !=0:\n",
        "            random_attr = np.array([df[LABELS].values[i] for i in np.random.randint(0,202600, self.num_img)])\n",
        "            generated_images = self.model.generator((random_latent_vectors, random_attr))\n",
        "        else:\n",
        "            generated_images = self.model.generator(random_latent_vectors)\n",
        "\n",
        "        generated_images.numpy()\n",
        "        if IMAGE_RANGE == '11':\n",
        "            generated_images = conv_range((-1,1), (0,1))(generated_images)\n",
        "\n",
        "        if ENABLE_WANDB:\n",
        "            log_images = [wandb.Image(img) for img in generated_images]\n",
        "            #log_name = f\"Epoch {str(epoch)+str(LABELS)+str(random_attr.numpy())}\"\n",
        "            wandb.log({f\"Epoch {epoch}\": (log_images)})\n",
        "\n",
        "        fig, axes = plt.subplots(1, self.num_img, figsize=(30,30))\n",
        "        \n",
        "        for i, axis in enumerate(axes):\n",
        "            axis.axis('off')\n",
        "            if N_ATTRIBUTES !=0:\n",
        "                axis.title.set_text('\\n'.join([(1-label)*'Not_'+LABELS[i]+' •'*label for i,label in enumerate(random_attr[i])]))\n",
        "            axis.imshow(generated_images[i])\n",
        "        plt.show()\n",
        " \n",
        "    \"\"\"def on_batch_end(self, batch, logs=None):\n",
        "\n",
        "        freq = 200\n",
        "        if (batch+1) % freq == 0:\n",
        "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "            random_attr = tf.random.uniform(shape=(self.num_img, N_ATTRIBUTES), maxval=2, dtype='int64')\n",
        "\n",
        "            generated_images = self.model.generator((random_latent_vectors, random_attr))\n",
        "            generated_images.numpy()\n",
        "\n",
        "\n",
        "            fig, axes = plt.subplots(1, self.num_img, figsize=(30,30))\n",
        "            for i, axis in enumerate(axes):\n",
        "                axis.axis('off')\n",
        "                axis.title.set_text(f\"{[LABELS[label] for label in random_attr[i]]}\")\n",
        "                if IMAGE_RANGE == '11':\n",
        "                    axis.imshow(generated_images[i]/2+0.5)\n",
        "                else:\n",
        "                    axis.imshow(generated_images[i])\n",
        "\n",
        "            fig.set_facecolor((56/255,56/255,56/255))\n",
        "            plt.show()\"\"\";"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bLvG6V2bbFA"
      },
      "source": [
        "### Model logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJU9s6nixbc6"
      },
      "source": [
        "# log checkpoint artifacts to wandb\n",
        "\n",
        "SAVE_PATH = 'models/'\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "class ModelLogger(keras.callbacks.Callback):\n",
        "    def __init__(self, model_name, run):\n",
        "        super(ModelLogger, self).__init__()\n",
        "        assert ENABLE_WANDB == True\n",
        "        self.model_name = model_name\n",
        "        self.run = run\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        self.model.save(SAVE_PATH + self.model_name, save_format='tf')\n",
        "        self.artifact = wandb.Artifact(self.model_name, type='model')\n",
        "        self.artifact.add_dir(SAVE_PATH)\n",
        "        self.run.log_artifact(self.artifact)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaAZrlqRnNe2"
      },
      "source": [
        "### FID Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InwVA_X6UPWt",
        "outputId": "ef1de38c-a80a-4167-bc44-7c3b28a7b5d6"
      },
      "source": [
        "#@title Load Multilabel Classifier Model{form-width: \"35%\", display-mode: \"both\" }\n",
        "\n",
        "#refactor to attribute accuracy logger\n",
        "ENABLE_LOAD = True #@param {type:\"boolean\"}\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "def f1_metric_logits(y_true, y_pred):\n",
        "    y_pred = K.round(keras.activations.sigmoid(y_pred))\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)\n",
        "\n",
        "\n",
        "def f1_metric(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)\n",
        "\n",
        "def weighted_bce_from_logits(y_true, y_pred):\n",
        "\n",
        "    #inverted_frequency weighting\n",
        "    pos_weight= (1-attribute_frequency)*2\n",
        "    y_true = tf.cast(y_true,'float32')\n",
        "    bce = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, pos_weight=pos_weight)\n",
        "    #weighted_bce = attribute_frequency*(1-y_true) *bce\n",
        "    #print(bce, weighted_bce)\n",
        "    return K.mean(bce,axis=-1)\n",
        "\n",
        "\n",
        "custom_objects = {'f1_metric': f1_metric, \n",
        "                  'f1_metric_logits': f1_metric_logits,\n",
        "                  'weighted_bce_from_logits': weighted_bce_from_logits}\n",
        "\n",
        "if IMAGE_SIZE == 64:\n",
        "    if N_ATTRIBUTES == 2:\n",
        "        resume_id = \"    '2ckbukxu'\"\n",
        "        model_name = experiment_name = \"mobilenet_2attr_weighted\"\n",
        "\n",
        "    elif N_ATTRIBUTES == 10:\n",
        "        resume_id = \"1ciqd5nw\"\n",
        "        model_name = experiment_name = \"mobilenet_10attr_weighted\"\n",
        "\n",
        "    elif N_ATTRIBUTES == 40:\n",
        "        resume_id = \"123ijj97\"\n",
        "        model_name = experiment_name = \"mobilenet_40attr_weighted\"\n",
        "\n",
        "    else:\n",
        "        print(\"N_ATTRIBUTES=\", N_ATTRIBUTES, \"skipping classifier load\")\n",
        "        ENABLE_LOAD = False\n",
        "elif IMAGE_SIZE == 218:\n",
        "    if N_ATTRIBUTES == 2:\n",
        "        resume_id = '3b15ld8p'\n",
        "        model_name = experiment_name = \"mobilenet_2attr_218x178_weighted\"\n",
        "\n",
        "    elif N_ATTRIBUTES == 10:\n",
        "        resume_id = \"3n56eda3\"\n",
        "        model_name = experiment_name = \"mobilenet_10attr_218x178_weighted\"\n",
        "    \n",
        "    elif N_ATTRIBUTES == 40:\n",
        "        resume_id = \"zd9xh3kq\"\n",
        "        model_name = experiment_name = \"mobilenet_40attr_218x178_weighted\"\n",
        "   \n",
        "    else:\n",
        "        print(\"N_ATTRIBUTES=\", N_ATTRIBUTES, \"skipping classifier load\")\n",
        "        ENABLE_LOAD = False\n",
        "\n",
        "\n",
        "if ENABLE_LOAD:\n",
        "    project_name = \"GAN\"\n",
        "    !pip install wandb > /dev/null\n",
        "    !wandb login\n",
        "    import wandb\n",
        "    from wandb.keras import WandbCallback\n",
        "    run = wandb.init(project=project_name, \n",
        "                     name=experiment_name, \n",
        "                     resume=resume_id)\n",
        "\n",
        "    run_name = 'buio/GAN/'+ model_name\n",
        "    artifact_run = run_name +':latest'\n",
        "\n",
        "    artifact = run.use_artifact(artifact_run, type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "    attribute_classifier = tf.keras.models.load_model(os.path.join(artifact_dir, model_name),\n",
        "                                                      custom_objects=custom_objects)\n",
        "\n",
        "    attribute_classifier.compile(loss=keras.losses.BinaryCrossentropy(), metrics='binary_accuracy')\n",
        "    #loss, gen_attr_accuracy = attribute_classifier.evaluate(dataset_df)\n",
        "    #gen_attr_accuracy\n",
        "\n",
        "    run.finish()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_ATTRIBUTES= 0 skipping classifier load\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ085awLWlo-"
      },
      "source": [
        "FID_COUNT = 10000\n",
        "class FIDLogger(keras.callbacks.Callback):\n",
        "    def __init__(self, real_embeddings, real_attr=None):\n",
        "        super(FIDLogger, self).__init__()\n",
        "        self.real_embeddings = real_embeddings\n",
        "        self.real_attr = real_attr\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None): # end\n",
        "\n",
        "        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "        if N_ATTRIBUTES !=0:\n",
        "            multi_evaluator = define_evaluator(self.model.generator, attribute_classifier)\n",
        "        else:\n",
        "            multi_evaluator = define_evaluator(self.model.generator)\n",
        "\n",
        "        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN)\n",
        "        \n",
        "        gen_size = 100\n",
        "        assert FID_COUNT % gen_size == 0\n",
        "\n",
        "        generated_embeddings = np.zeros((FID_COUNT, 2048), dtype='float32')\n",
        "\n",
        "        if N_ATTRIBUTES !=0:\n",
        "            pred_attr = np.zeros((FID_COUNT, N_ATTRIBUTES), dtype='float32')\n",
        "\n",
        "            for i in tqdm(range(FID_COUNT//gen_size)):\n",
        "              \n",
        "                batch_embeddings, batch_attr = multi_evaluator((\n",
        "                    tf.random.normal([gen_size, LATENT_DIM]),\n",
        "                    self.real_attr[i*gen_size:(i+1)*gen_size]))\n",
        "                \n",
        "                generated_embeddings[i*gen_size:(i+1)*gen_size] = batch_embeddings\n",
        "                pred_attr[i*gen_size:(i+1)*gen_size] = batch_attr\n",
        "\n",
        "            acc = np.mean(tf.keras.metrics.binary_accuracy(self.real_attr, pred_attr))\n",
        "            f1_score = f1_metric(self.real_attr, pred_attr).numpy()\n",
        "\n",
        "        else:\n",
        "            for i in tqdm(range(FID_COUNT//gen_size)):\n",
        "              \n",
        "                batch_embeddings = multi_evaluator(tf.random.normal([gen_size, LATENT_DIM]))\n",
        "                \n",
        "                generated_embeddings[i*gen_size:(i+1)*gen_size] = batch_embeddings\n",
        "\n",
        "        fid = compute_fid(self.real_embeddings, generated_embeddings)\n",
        "\n",
        "        if N_ATTRIBUTES !=0:\n",
        "            print(\"FID:\",fid, \"attribute accuracy:\", acc, \"f1-score:\", f1_score)\n",
        "        else:\n",
        "            print(\"FID:\",fid)\n",
        "\n",
        "        if ENABLE_WANDB:\n",
        "            wandb.log({'FID': fid})\n",
        "            if N_ATTRIBUTES !=0:\n",
        "                wandb.log({'attr_acc': acc})\n",
        "                wandb.log({'attr_f1_score': f1_score})\n",
        "\n",
        "def define_evaluator(generator, attr_classifier=None):\n",
        "    \n",
        "    img = generator.output\n",
        "    img = tf.cast(img, tf.float32)\n",
        "\n",
        "    if IMAGE_RANGE == '01':\n",
        "        img = img*2 -1\n",
        "\n",
        "    img_up299 = tf.image.resize(img, (299, 299), method='bilinear', antialias=True, name='upsample_bilinear')\n",
        "    output_repr = inception_model(img_up299)\n",
        "\n",
        "    if N_ATTRIBUTES !=0:\n",
        "        attributes = attr_classifier(img) # mobilenet model does resizing.. does *2-1 also...\n",
        "        multi_evaluator = tf.keras.Model(inputs=generator.inputs, outputs=[output_repr, attributes])\n",
        "    else:\n",
        "        multi_evaluator = tf.keras.Model(inputs=generator.inputs, outputs=output_repr)\n",
        "\n",
        "    return multi_evaluator\n",
        "\n",
        "def compute_fid(real_embeddings, generated_embeddings):\n",
        "    # compute mean and covariance statistics\n",
        "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
        "    mu2, sigma2 = generated_embeddings.mean(axis=0), np.cov(generated_embeddings, rowvar=False)\n",
        "    # compute sum squared difference between means\n",
        "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "    # compute sqrt of product between cov\n",
        "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
        "    # check and correct imaginary numbers from sqrt\n",
        "    if np.iscomplexobj(covmean):\n",
        "      covmean = covmean.real\n",
        "    # compute fid score\n",
        "    trace = np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    fid = ssdiff + trace\n",
        "    print(\"fid:\",fid,\"ssdiff:\",ssdiff, \"trace:\",trace)\n",
        "    return fid"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLB5T7B6KV5j"
      },
      "source": [
        "### Precompute FID embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c5d76795b0ec431494e9853500d96f76",
            "c6a57ebec99a49609866f76619fe81d8",
            "30705ca4dcc44469b7e2b5ea0f5d71d3",
            "2518168f4217428fbff631ce1f57dbb2",
            "02cf7dc81a0445fa80e20d1f634ad828",
            "b8bd26ccd56e4c0fa4332683282acd59",
            "9545e53a0fea491e8b1fc81d317754f3",
            "06f0beb281ea471faa1fe0b8e36c69de",
            "9e400e2f4aca4ec9bfd8833977bce304",
            "f467281483674c5bb890d8df97bc9f6a",
            "bd9ee7ca5b0a4431a1298c2ec536609b"
          ]
        },
        "id": "Adnl87j8MWGL",
        "outputId": "992f53c8-a041-4a1d-ef89-37282a2cd8db"
      },
      "source": [
        "#fid\n",
        "inception_model = InceptionV3(include_top=False, \n",
        "                              weights=\"imagenet\", \n",
        "                              pooling='avg',\n",
        "                              input_shape=(299, 299, 3))\n",
        "\n",
        "\n",
        "def define_real_prep():\n",
        "\n",
        "    img = layers.Input(shape=IMAGE_SHAPE, name='img_input')\n",
        "    img_up = tf.image.resize(img, (299, 299), method='bilinear', antialias=True, name='upsample_bilinear')\n",
        "    if IMAGE_RANGE == '01':\n",
        "        img_up = img_up*2 -1\n",
        "\n",
        "    output = inception_model(img_up)\n",
        "\n",
        "    real_prep = tf.keras.Model(inputs=[img], outputs=[output])\n",
        "\n",
        "    return real_prep\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "real_prep = define_real_prep()\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN)\n",
        "\n",
        "fid_bs = 200\n",
        "assert FID_COUNT % fid_bs == 0\n",
        "\n",
        "dataset_fid = dataset_train.unbatch().batch(fid_bs)\n",
        "\n",
        "real_embeddings = np.zeros((FID_COUNT, 2048), dtype='float32')\n",
        "if N_ATTRIBUTES !=0:\n",
        "    real_attr = np.zeros((FID_COUNT, N_ATTRIBUTES), dtype='float32')\n",
        "\n",
        "for i in tqdm(range(FID_COUNT//fid_bs)):\n",
        "    if N_ATTRIBUTES !=0:\n",
        "        batch_img, batch_attr = next(iter(dataset_fid))\n",
        "        real_attr[i*fid_bs:(i+1)*fid_bs] = batch_attr\n",
        "    else:\n",
        "        batch_img = next(iter(dataset_fid))\n",
        "    batch_embeddings = real_prep(batch_img)\n",
        "    real_embeddings[i*fid_bs:(i+1)*fid_bs] = batch_embeddings\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d76795b0ec431494e9853500d96f76",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTX1EZbYLy2s"
      },
      "source": [
        "# ⏳ Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gXF5AnjVyQS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350,
          "referenced_widgets": [
            "1cd4464d5e3c44acae2b9eff48ee1e51",
            "cdb2c9b2fd9e4aacbb161802e9da2983",
            "c0d9305e01f749b58ac904b116d810ba",
            "beb2f93b18144f35bcc5cd3333b3f61c",
            "f5e50e9e5a1b465eb4d6ff50fadf8dae",
            "71d1dce1b4ab4c588d0c523dd72c15b5",
            "991f09e22e6d48fd94c0cd4299db450e",
            "a2be1fd30b1b44bca580c039c6b05771"
          ]
        },
        "outputId": "bd293ae0-d7b3-4388-bbd9-d7aca5742785"
      },
      "source": [
        "ADAM_LR_D = 0.00005\n",
        "ADAM_LR_G = 0.00005\n",
        "ADAM_B1 = 0.5\n",
        "ADAM_B2 = 0.999\n",
        "\n",
        "ENABLE_WANDB = True #@param {type:\"boolean\"}\n",
        "WANDB_RESUME = False #@param {type:\"boolean\"}\n",
        "\n",
        "resume_id = \"h0hgqpg9\" #@param {type: \"string\"}\n",
        "WANDB_RESUME = (resume_id if WANDB_RESUME else False)\n",
        "\n",
        "entity = 'buio'\n",
        "project_name = \"GAN\" #@param {type: \"string\"}\n",
        "model_name = \"dcgan\" #@param {type: \"string\"}\n",
        "experiment_name = \"dcgan\" #@param {type: \"string\"}\n",
        "run_notes = \"\" #@param {type: \"string\"}\n",
        "\n",
        "assert '-' not in model_name\n",
        "assert ' ' not in model_name\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "if ENABLE_WANDB:\n",
        "    !pip install wandb > /dev/null\n",
        "    !wandb login\n",
        "    import wandb\n",
        "    from wandb.keras import WandbCallback\n",
        "    run = wandb.init(project=project_name, \n",
        "                     entity = entity,\n",
        "                     name=experiment_name, \n",
        "                     resume=WANDB_RESUME,\n",
        "                     save_code=True)\n",
        "    if run_notes:\n",
        "        wandb.notes = run_notes\n",
        "    wandb.config.batch_size = BATCH_SIZE\n",
        "    wandb.config.adam_lr_d = ADAM_LR_D\n",
        "    wandb.config.adam_lr_g = ADAM_LR_G\n",
        "    wandb.config.adam_b1 = ADAM_B1\n",
        "    wandb.config.adam_b2 = ADAM_B2\n",
        "    wandb.config.img_range = IMAGE_RANGE\n",
        "    wandb.config.n_attributes = N_ATTRIBUTES"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbuio\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:2c17w3q3) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 2688... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cd4464d5e3c44acae2b9eff48ee1e51",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 834.47MB of 834.47MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>FID</td><td>▁</td></tr><tr><td>d_loss</td><td>█▄▃▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>g_loss</td><td>▁▂▄█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>FID</td><td>459.62257</td></tr><tr><td>d_loss</td><td>0.30869</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>g_loss</td><td>1.01555</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 9 media file(s), 14 artifact file(s) and 1 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">dcgan</strong>: <a href=\"https://wandb.ai/buio/GAN/runs/2c17w3q3\" target=\"_blank\">https://wandb.ai/buio/GAN/runs/2c17w3q3</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211022_180130-2c17w3q3/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:2c17w3q3). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/buio/GAN/runs/t1qyadzp\" target=\"_blank\">dcgan</a></strong> to <a href=\"https://wandb.ai/buio/GAN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "627UA8yuH9M_"
      },
      "source": [
        "## Build the ACGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "Hq3HjDpiRkyY",
        "outputId": "9556610f-9bb5-403f-a100-37ffab66c392"
      },
      "source": [
        "LATENT_DIM = 100\n",
        "\n",
        "if IMAGE_SHAPE == (64,64,3):\n",
        "    discriminator = define_discriminator64(n_attributes=N_ATTRIBUTES)\n",
        "    generator = define_generator64(latent_dim=LATENT_DIM, n_attributes=N_ATTRIBUTES)\n",
        "elif IMAGE_SHAPE == (218,178,3):\n",
        "    discriminator = define_discriminator218(n_attributes=N_ATTRIBUTES)\n",
        "    generator = define_generator218(latent_dim=LATENT_DIM, n_attributes=N_ATTRIBUTES)\n",
        " \n",
        "\n",
        "if N_ATTRIBUTES !=0:\n",
        "    gan = ACGAN(discriminator, generator, latent_dim=LATENT_DIM, n_attributes=N_ATTRIBUTES)\n",
        "else:\n",
        "    gan = GAN(discriminator, generator, latent_dim=LATENT_DIM)\n",
        "\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=ADAM_LR_D, beta_1=ADAM_B1, beta_2 = ADAM_B2),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=ADAM_LR_G, beta_1=ADAM_B1, beta_2 = ADAM_B2),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "# build model for saving\n",
        "\n",
        "random_latent_vectors = tf.random.normal(shape=(1, LATENT_DIM))\n",
        "if N_ATTRIBUTES !=0:\n",
        "    random_attr = np.array([df[LABELS].values[i] for i in np.random.randint(0,202600, 1)])\n",
        "    #random_attr = np.zeros((1,N_ATTRIBUTES))\n",
        "\n",
        "    images, d_loss = gan.predict((random_latent_vectors, random_attr))\n",
        "\n",
        "    plt.title('\\n'.join([(1-label)*'Not_'+LABELS[i]+' •'*label for i,label in enumerate(random_attr[0])]))\n",
        "else:\n",
        "    images, d_loss = gan.predict(random_latent_vectors)\n",
        "\n",
        "print(images.shape, np.min(images), np.max(images))\n",
        "plt.imshow(images[0]/2+0.5)\n",
        "#gan.save('models/'+model_name+experiment_name, save_format='tf')\n",
        "resume_epoch = 0"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 64, 64, 3) -0.15460944 0.13688615\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgmKY6jDdvjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38e0d9f-d79e-4ed4-d486-d6f2ad7627d4"
      },
      "source": [
        "if WANDB_RESUME:\n",
        "    run_name = entity+'/GAN/'+ model_name + experiment_name\n",
        "    artifact_run = run_name +':latest'\n",
        "\n",
        "    artifact = run.use_artifact(artifact_run, type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "    gan.load_weights(os.path.join(artifact_dir, model_name + experiment_name))\n",
        "    resume_epoch = int(artifact_dir.split(':v')[-1]) +1\n",
        "else:\n",
        "    resume_epoch = 0\n",
        "\n",
        "resume_epoch"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgT4VqgKJx-7"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaES7uhMhfk-",
        "outputId": "7bb4eb3c-67f4-477d-d648-081b426c62b3"
      },
      "source": [
        "epochs = 60 \n",
        "\n",
        "if ENABLE_WANDB:\n",
        "    callbacks = [\n",
        "        WandbCallback(log_batch_frequency=10, verbose=1),\n",
        "        ImagesLogger(num_img=8, latent_dim=LATENT_DIM),\n",
        "        ModelLogger(model_name+experiment_name, run=run)]\n",
        "\n",
        "    if N_ATTRIBUTES !=0:\n",
        "        callbacks.append(FIDLogger(real_embeddings, real_attr))\n",
        "    else:\n",
        "        callbacks.append(FIDLogger(real_embeddings))\n",
        "\n",
        "history = gan.fit(dataset_train, epochs=epochs,\n",
        "        initial_epoch=resume_epoch, \n",
        "        #steps_per_epoch=20,\n",
        "        callbacks = callbacks\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            " 327/3166 [==>...........................] - ETA: 14:25 - d_loss: 0.1809 - g_loss: 4.5714"
          ]
        }
      ]
    }
  ]
}